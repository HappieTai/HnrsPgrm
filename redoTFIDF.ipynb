{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b722a4b0-d9b5-44d6-987b-c26a39f42120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect import LangDetectException\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcbdb829-c5ed-4dcb-bc4a-7022e70e3b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tal012/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tal012/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/tal012/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/tal012/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "938c53cc-7412-4828-8521-15a10a61be48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/tal012/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85073dfd-acc4-4592-849c-4056fa5dd59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    # Iterate over each page\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4f55e07c-7e99-46fa-9e8b-a0d9a8b6a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#textbooks from pdf form:\n",
    "phili_7 = extract_text_from_pdf('phillipeans/Phili_7.pdf')\n",
    "phili_8 = extract_text_from_pdf('phillipeans/phili_8.pdf')\n",
    "phili_9 = extract_text_from_pdf('phillipeans/Phili_9.pdf')\n",
    "#india_7 = extract_text_from_pdf('india/India_7.pdf')\n",
    "india_8 = extract_text_from_pdf('india/India_8.pdf')\n",
    "india_9 = extract_text_from_pdf('india/India_9.pdf')\n",
    "china_7 = extract_text_from_pdf('china/China_7.pdf')\n",
    "china_8 = extract_text_from_pdf('china/china_8.pdf')\n",
    "china_9 = extract_text_from_pdf('china/china_9.pdf')\n",
    "turkey_9 = extract_text_from_pdf('turkey/Turkey_9.pdf')\n",
    "tun_8 = extract_text_from_pdf('Tunisia/Tun_8.pdf')\n",
    "tun_9 = extract_text_from_pdf('Tunisia/Tun_9.pdf')\n",
    "saudi_7 = extract_text_from_pdf('saudi/Saudi_7.pdf')\n",
    "saudi_8 = extract_text_from_pdf('saudi/Saudi_8.pdf')\n",
    "saudi_9 = extract_text_from_pdf('saudi/Saudi_9.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6d537428-a566-4964-85c2-f175846b692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vietnam/Viet_7.txt','r')as file:\n",
    "    viet_7 = file.read()\n",
    "with open('vietnam/Viet_8.txt', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    viet_8 = file.read()\n",
    "with open('mexico/Mexico_7.txt','r', encoding = 'utf-8', errors = 'ignore') as file:\n",
    "    mexico_7 = file.read()\n",
    "with open('mexico/Mexico_8.txt','r', encoding = 'utf-8', errors = 'ignore') as file:\n",
    "    mexico_8 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "37f03d42-1483-4c2c-a65f-7fb6199ac108",
   "metadata": {},
   "outputs": [],
   "source": [
    "textbooks = [phili_7, phili_8, phili_9, \n",
    "             #india_7, \n",
    "             india_8, india_9, \n",
    "             china_7, china_8, china_9, \n",
    "             turkey_9, \n",
    "             tun_8, tun_9, \n",
    "             saudi_7,saudi_8, saudi_9,\n",
    "            viet_7, viet_8,\n",
    "            mexico_7, mexico_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1b78f8fb-4f1f-4906-b696-ccab12fede65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(post):\n",
    "    if detect(post) != 'en':\n",
    "        return ''\n",
    "    # Make posts lowercase\n",
    "    post = post.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    post = re.sub(r'[^a-zA-Z\\s]', ' ', post)\n",
    "    \n",
    "    # Remove words with repeated letters\n",
    "    #post = re.sub(r'([a-zA-Z])\\1+', r'\\1', post)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(post)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3c171b97-991c-401c-a4ea-f07bb763a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTextbooks = []\n",
    "for i in textbooks:\n",
    "    cleanTextbooks.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "48194567-aed4-4794-9489-130ed212eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "menWords = [\" man \", \" boy \", \" male \", \" brother \", \" father \", \" son \", \" husband \", \" king \", \" prince \", \" uncle \", \" nephew \", \" he \", \" him \", \" his \", \" gentleman \", \" sir \", \" mr. \", \" hero \", \" lord \", \" patriarch \", \" men \"]\n",
    "\n",
    "womenWords = [\" woman \", \" girl \", \" female \", \" sister \", \" mother \", \" daughter \", \" wife \", \" queen \", \" princess \", \" aunt \", \" niece \", \" she \", \" her \", \" hers \", \" lady \", \" ma'am \",\" madam \", \" mrs. \", \" ms. \", \" miss \", \" heroine \", \" dame \", \" matriarch \", \" women \"]\n",
    "\n",
    "menWordsNoSpace = [\"man\", \"boy\", \"male\", \"brother\", \"father\", \"son\", \"husband\", \"king\", \"prince\", \"uncle\", \"nephew\", \"he\", \"him\", \"his\", \"gentleman\", \"sir\", \"mr\", \"hero\", \"lord\", \"patriarch\", \"men\"]\n",
    "womenWordsNoSpace = [\"woman\", \"girl\", \"female\", \"sister\", \"mother\", \"daughter\", \"wife\", \"queen\", \"princess\", \"aunt\", \"niece\", \"she\", \"her\", \"hers\", \"lady\", \"ma'am\",\"madam\", \"mrs\", \"heroine\", \"dame\", \"matriarch\", \"women\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d9b39258-8c6a-454f-92e4-a9acc9aade0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaned the textbooks before getting the gendered words\n",
    "menPara = [[] for _ in range(18)]\n",
    "womenPara = [[] for _ in range(18)]\n",
    "for i in range(0, 18):\n",
    "    text = cleanTextbooks[i]\n",
    "    paragraphs = [text[j:j+100] for j in range(0, len(text), 100)]\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        for j in menWords:\n",
    "            if j in paragraph.lower():  # Converts paragraph to lowercase for case-insensitive search\n",
    "                menPara[i].append(paragraph)\n",
    "        for j in womenWords:\n",
    "            if j in paragraph.lower():  # Converts paragraph to lowercase for case-insensitive search\n",
    "                womenPara[i].append(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "42c9dc96-3e08-487c-8736-fe6766e27f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "160e23c4-7443-421c-a1b6-3e1796b7ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobeTFIDF = [\" \".join(\" \".join(row) for row in menPara),\" \".join(\" \".join(row) for row in womenPara)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "db1eaaf7-273b-4820-a9d6-f3fcea2e3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = vectorizer.fit_transform(tobeTFIDF)\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense representation for easy interpretation\n",
    "dense_tfidf = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "df0f1ed0-aa54-4fa2-9834-280f5099cafd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get TF-IDF scores for menPara (Document 0) or womenPara (Document 1)\n",
    "doc_index = 0  # Change to 1 for womenPara\n",
    "doc_tfidf_scores = dense_tfidf[doc_index]\n",
    "\n",
    "# Pair each word with its TF-IDF score\n",
    "word_score_pairs = list(zip(feature_names, doc_tfidf_scores))\n",
    "\n",
    "# Sort the words by TF-IDF score in descending order\n",
    "sorted_word_scores = sorted(word_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top 100 words\n",
    "top_100_words_ms = sorted_word_scores[:100]\n",
    "top_100_words_m = [word for word, score in sorted_word_scores[:100]]\n",
    "\n",
    "\n",
    "# Print the top 100 words with their TF-IDF scores\n",
    "#print(f\"Top 100 TF-IDF words for Document {doc_index}:\")\n",
    "#for word, score in top_100_words:\n",
    "    #print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b81d580b-0abb-4070-9b2d-8153e0c5e35b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get TF-IDF scores for menPara (Document 0) or womenPara (Document 1)\n",
    "doc_index = 1  # Change to 1 for womenPara\n",
    "doc_tfidf_scores = dense_tfidf[doc_index]\n",
    "\n",
    "# Pair each word with its TF-IDF score\n",
    "word_score_pairs = list(zip(feature_names, doc_tfidf_scores))\n",
    "\n",
    "# Sort the words by TF-IDF score in descending order\n",
    "sorted_word_scores = sorted(word_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top 100 words\n",
    "top_100_words_fs = sorted_word_scores[:100]\n",
    "top_100_words_f = [word for word, score in sorted_word_scores[:100]]\n",
    "\n",
    "\n",
    "# Print the top 100 words with their TF-IDF scores\n",
    "#print(f\"Top 100 TF-IDF words for Document {doc_index}:\")\n",
    "#for word, score in top_100_words:\n",
    "    #print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "882595fe-fc84-4705-8a19-379d0bbb9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "common = list(set(top_100_words_f) & set(top_100_words_m))\n",
    "len(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c6457229-fafa-482c-8e6b-e3bf17fef6d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_100_words_f = [word for word in top_100_words_f if word not in common]\n",
    "top_100_words_f = [word for word in top_100_words_f if word not in womenWordsNoSpace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "34a0a010-c7f2-4194-8c1c-7a8018c81023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_100_words_m = [word for word in top_100_words_m if word not in common]\n",
    "top_100_words_m = [word for word in top_100_words_m if word not in menWordsNoSpace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "95094d63-266b-473c-a19a-dc600674d4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'femaleTFIDF' has been created.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "filename = 'femaleTFIDF'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow(['Word'])\n",
    "    \n",
    "    # Write each word in the list as a row\n",
    "    for word in top_100_words_f:\n",
    "        writer.writerow([word])\n",
    "\n",
    "print(f\"CSV file '{filename}' has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3a7b2f87-1df1-4814-9f99-73f0a8d16103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'maleTFIDF' has been created.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "filename = 'maleTFIDF'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow(['Word'])\n",
    "    \n",
    "    # Write each word in the list as a row\n",
    "    for word in top_100_words_m:\n",
    "        writer.writerow([word])\n",
    "\n",
    "print(f\"CSV file '{filename}' has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d85e72-8d2a-4e5d-9eef-af30ff49a72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
