{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db67ba19-129b-4485-b5f8-7737f1eb8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "def pdf_extract(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "turk = pdf_extract('turkey/Turkey_9.pdf')\n",
    "bela = pdf_extract('belarus/belarus_9.pdf')\n",
    "ukr = pdf_extract('Ukraine/ukraine_7.pdf')+\" \"+pdf_extract('Ukraine/ukraine_8.pdf')+\" \"+pdf_extract('Ukraine/ukraine_9.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e0eb76-c444-4655-a00e-c71efa2b5f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "textbooks = [turk, bela, ukr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d698d9f-9282-4938-9e6b-bdbbb58f3707",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c731351b-f590-47fd-a2da-141307297ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langdetect import detect\n",
    "from langdetect import LangDetectException\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048f653e-08fd-4854-9363-111be2beac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(post):\n",
    "    if detect(post) != 'en':\n",
    "        return ''\n",
    "    # Make posts lowercase\n",
    "    post = post.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    post = re.sub(r'[^a-zA-Z\\s]', ' ', post)\n",
    "    \n",
    "    # Remove words with repeated letters\n",
    "    #post = re.sub(r'([a-zA-Z])\\1+', r'\\1', post)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(post)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26899fb3-3fcb-4e08-a820-846fc13be753",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTextbooks = []\n",
    "for i in textbooks:\n",
    "    cleanTextbooks.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0865f5-c5a6-4171-b347-e2f7d84c1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These have spaces, we use these to select the male and female words\n",
    "menWords = [\" man \", \" boy \", \" male \", \" brother \", \" father \", \" son \", \" husband \", \" king \", \" prince \", \" uncle \", \" nephew \", \" he \", \" him \", \" his \", \" gentleman \", \" sir \", \" mr. \", \" hero \", \" lord \", \" patriarch \", \" men \"]\n",
    "womenWords = [\" woman \", \" girl \", \" female \", \" sister \", \" mother \", \" daughter \", \" wife \", \" queen \", \" princess \", \" aunt \", \" niece \", \" she \", \" her \", \" hers \", \" lady \", \" ma'am \",\" madam \", \" mrs. \", \" ms. \", \" miss \", \" heroine \", \" dame \", \" matriarch \", \" women \"]\n",
    "#These have no spaces, so these can be used to throw away words during TFIDF\n",
    "menWordsNoSpace = [\"man\", \"boy\", \"male\", \"brother\", \"father\", \"son\", \"husband\", \"king\", \"prince\", \"uncle\", \"nephew\", \"he\", \"him\", \"his\", \"gentleman\", \"sir\", \"mr\", \"hero\", \"lord\", \"patriarch\", \"men\"]\n",
    "womenWordsNoSpace = [\"woman\", \"girl\", \"female\", \"sister\", \"mother\", \"daughter\", \"wife\", \"queen\", \"princess\", \"aunt\", \"niece\", \"she\", \"her\", \"hers\", \"lady\", \"ma'am\",\"madam\", \"mrs\", \"heroine\", \"dame\", \"matriarch\", \"women\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "290d69e4-4333-40ca-87ba-568c25c479e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "menPara = [[] for _ in range(3)]\n",
    "womenPara = [[] for _ in range(3)]\n",
    "for i in range(0, 3):\n",
    "    text = cleanTextbooks[i]\n",
    "    paragraphs = [text[j:j+100] for j in range(0, len(text), 100)]\n",
    "    for paragraph in paragraphs:\n",
    "        for j in menWords:\n",
    "            if j in paragraph:\n",
    "                menPara[i].append(paragraph)\n",
    "        for j in womenWords:\n",
    "            if j in paragraph:\n",
    "                womenPara[i].append(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a244f6af-3406-4451-90f3-cc6888590dcc",
   "metadata": {},
   "source": [
    "# Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2334899-c275-4195-8f8a-e04226d6e94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n",
      "342\n"
     ]
    }
   ],
   "source": [
    "men_count = sum(len(sublist) for sublist in menPara)\n",
    "print(men_count)\n",
    "women_count = sum(len(sublist) for sublist in womenPara)\n",
    "print(women_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db2fb31-e585-4ab5-b235-8fef226dbcba",
   "metadata": {},
   "source": [
    "# Firstness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc06c38f-9e71-45fe-8d94-c76c27ae5545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [('mother', 'father'), ('queen', 'man'), ('queen', 'man'), ('daughter', 'mr'), ('daughter', 'mr')]\n"
     ]
    }
   ],
   "source": [
    "femaleFirst = []\n",
    "for sublist in womenPara:  \n",
    "    for paragraph in sublist:  \n",
    "        if isinstance(paragraph, str):  \n",
    "            words = paragraph.split()  \n",
    "            for i in range(len(words) - 1): \n",
    "                if words[i].lower() in womenWordsNoSpace and words[i + 1].lower() in menWordsNoSpace:\n",
    "                    femaleFirst.append((words[i], words[i + 1]))\n",
    "print(len(femaleFirst),femaleFirst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b2b3f0a-1032-438d-a825-8a96727a06ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 [('brother', 'sister'), ('brother', 'sister'), ('brother', 'sister'), ('boy', 'girl'), ('men', 'women'), ('male', 'female'), ('male', 'female'), ('son', 'daughter'), ('son', 'daughter'), ('son', 'daughter'), ('son', 'daughter'), ('male', 'female'), ('male', 'female'), ('brother', 'sister'), ('nephew', 'niece'), ('sir', 'madam'), ('sir', 'madam'), ('mr', 'mrs'), ('nephew', 'niece'), ('nephew', 'niece'), ('nephew', 'niece'), ('sir', 'madam'), ('father', 'mother'), ('man', 'wife'), ('man', 'woman'), ('men', 'girl')]\n"
     ]
    }
   ],
   "source": [
    "maleFirst = []\n",
    "for sublist in menPara:  \n",
    "    for paragraph in sublist:  \n",
    "        if isinstance(paragraph, str):  \n",
    "            words = paragraph.split()  \n",
    "            for i in range(len(words) - 1):  \n",
    "                if words[i].lower() in menWordsNoSpace and words[i + 1].lower() in womenWordsNoSpace:\n",
    "                    maleFirst.append((words[i], words[i + 1]))\n",
    "print(len(maleFirst),maleFirst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af37994e-05be-412d-9bb3-04d6d55624ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sameLevel = {\n",
    "    \"siblings\": {\"brother\", \"sister\"},\n",
    "    \"parent_child\": {\"father\", \"mother\", \"son\", \"daughter\"},\n",
    "    \"spouses\": {\"husband\", \"wife\"},\n",
    "    \"royalty\": {\"king\", \"queen\", \"prince\", \"princess\"},\n",
    "    \"extended_family\": {\"uncle\", \"aunt\", \"nephew\", \"niece\"},\n",
    "    \"personal_pronouns\": {\"he\", \"she\", \"him\", \"her\", \"his\", \"hers\"},\n",
    "    \"titles\": {\"gentleman\", \"lady\", \"sir\", \"ma'am\", \"mr\", \"mrs\", \"madam\"},\n",
    "    \"heroic_roles\": {\"hero\", \"heroine\"},\n",
    "    \"social_roles\": {\"patriarch\", \"matriarch\"},\n",
    "    \"general_plural\": {\"men\", \"women\"},\n",
    "}\n",
    "flat = {word for group in sameLevel.values() for word in group}\n",
    "def isSame(pair):\n",
    "    return pair[0] in flat and pair[1] in flat\n",
    "femaleFirst = [pair for pair in femaleFirst if isSame(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfbab8b1-a034-45fb-b4bb-ed631ff7bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "maleFirst = [pair for pair in maleFirst if isSame(pair)]\n",
    "print(len(maleFirst))\n",
    "print(len(femaleFirst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42749a-7ebd-4437-a0c0-41fc656e0b60",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb0da50-bff0-4643-8b34-a47567b2d4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.095, 'neu': 0.722, 'pos': 0.183, 'compound': 1.0} {'neg': 0.07, 'neu': 0.733, 'pos': 0.197, 'compound': 1.0}\n"
     ]
    }
   ],
   "source": [
    "fm = ', '.join(map(str, [item for sublist in menPara for item in sublist]))\n",
    "fw = ', '.join(map(str, [item for sublist in womenPara for item in sublist]))\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "fscores = analyzer.polarity_scores(fw)\n",
    "mscores = analyzer.polarity_scores(fm)\n",
    "print(fscores,mscores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b56f4b6-b38a-4060-b0d6-6cb38e185d86",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b92c3597-3e9b-43c2-9309-060dc953377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect import LangDetectException\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46939507-4cb2-4878-bf59-dd490ff40646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming the words\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "def stem_nested_list(nested_list):\n",
    "    return [\n",
    "        [\n",
    "            ' '.join(stemmer.stem(word) for word in sentence.split())\n",
    "            for sentence in sublist\n",
    "        ]\n",
    "        for sublist in nested_list\n",
    "    ]\n",
    "menParaOriginal = menPara\n",
    "womenParaOriginal = womenPara\n",
    "menPara = stem_nested_list(menPara)\n",
    "womenPara = stem_nested_list(womenPara)\n",
    "fm = ', '.join(map(str, [item for sublist in menPara for item in sublist]))\n",
    "fw = ', '.join(map(str, [item for sublist in womenPara for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0455810b-6118-4b49-9f9b-fad88d160e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobeTFIDF = [\" \".join(\" \".join(row) for row in menPara),\" \".join(\" \".join(row) for row in womenPara)]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(tobeTFIDF)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense_tfidf = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41496c61-bcae-47b8-bd80-93cd15193b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf of female words\n",
    "doc_index = 1\n",
    "doc_tfidf_scores = dense_tfidf[doc_index]\n",
    "word_score_pairs = list(zip(feature_names, doc_tfidf_scores))\n",
    "sorted_word_scores = sorted(word_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "top_300_words_fs = sorted_word_scores[:300]\n",
    "top_300_words_f = [word for word, score in sorted_word_scores[:300]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3180e23a-9366-4c8a-8ea8-7f0614e5a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf of male words\n",
    "doc_index = 0  \n",
    "doc_tfidf_scores = dense_tfidf[doc_index]\n",
    "word_score_pairs_m = list(zip(feature_names, doc_tfidf_scores))\n",
    "sorted_word_scores_m = sorted(word_score_pairs_m, key=lambda x: x[1], reverse=True)\n",
    "top_300_words_ms = sorted_word_scores_m[:300]\n",
    "top_300_words_m = [word for word, score in sorted_word_scores_m[:300]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7caf800-9276-4c28-b948-7fe71e47ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove common words and gendered keywords\n",
    "common = list(set(top_300_words_f) & set(top_300_words_m))\n",
    "top_300_words_f = [word for word in top_300_words_f if word not in common]\n",
    "top_300_words_f = [word for word in top_300_words_f if word not in womenWordsNoSpace]\n",
    "female_300 = \" \".join(top_300_words_f)\n",
    "top_300_words_m = [word for word in top_300_words_m if word not in common]\n",
    "top_300_words_m = [word for word in top_300_words_m if word not in menWordsNoSpace]\n",
    "male_300 = \" \".join(top_300_words_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "359c200d-8b7e-4d88-9768-d95d1ecf33b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dexter wright ladi phone interview wear dear wallac wilkin agent lawyer dawn fred home wonder elizabeth gibbon never munro knit letter met night student told tom watch enter mi perfum wig wild bag captain femal john project seen stay book children complet crew eight enough four hotel kill ls nice record ship televis design dobson guy law mobil om tomorrow winter agatha amanda becam begin britain court depart di drink en end fli happi high ho holiday hudson id kitchen lennon librari littl mean messag michael moment monday number open park person rd read return show sport step talk town victoria word yet at colleg convers corner divorc dr fall fell finish foster germani han horac itali joke krugerand lamb lifeboat marcel marion melani olymp pari pen pretti prison pro quarter samuel scarv sock steak strathclyd susan taylor terribl wa whiski wind within wrote abl anna answer bank beauti\n",
      "worth jame charl gurney watt bert mari ben gold catherin news waiter nd ten salli took marpl play fiona sgt ticket way alan back hour stare big ltd roll bottl cox jane list sergeant store anoth assist black five major road save bay champion edgar gave joseph kaz khan koca lester martha matter menu price princ search sorin thoma tl understand weather adam better countri happen interest listen lynn mike rich shop special start stori wed armi best birthday bring celia collect cousin decid favourit gone hmm lo might nobodi norman noth order sea second send shall sinc sit six size suddenli teacher theatr togeth travel us white www alex appear behind bite break briefcas burger franc hogg howev ice idea kept lock lon mechan oz perhap permiss policeman rate sad sat seven side steward tristan true actor age attack believ born british buy church cloth crash excus feel\n"
     ]
    }
   ],
   "source": [
    "print(female_300)\n",
    "print(male_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02bb46d5-079f-4682-8bb8-a609b125e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "mwords = male_300.split()\n",
    "filteredMwords = [word for word in mwords if word in fm]\n",
    "wordMcounts = Counter(filteredMwords)\n",
    "topMwords = wordMcounts.most_common(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb6e9164-4aaf-4e46-9cb0-818099905043",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwords = female_300.split()\n",
    "filteredFwords = [word for word in fwords if word in fw]\n",
    "wordFcounts = Counter(filteredFwords)\n",
    "topFwords = wordFcounts.most_common(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad186a1a-666b-4bc3-89f8-fb43847cfc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 40 Male Words: [('worth', 1), ('jame', 1), ('charl', 1), ('gurney', 1), ('watt', 1), ('bert', 1), ('mari', 1), ('ben', 1), ('gold', 1), ('catherin', 1), ('news', 1), ('waiter', 1), ('nd', 1), ('ten', 1), ('salli', 1), ('took', 1), ('marpl', 1), ('play', 1), ('fiona', 1), ('sgt', 1), ('ticket', 1), ('way', 1), ('alan', 1), ('back', 1), ('hour', 1), ('stare', 1), ('big', 1), ('ltd', 1), ('roll', 1), ('bottl', 1), ('cox', 1), ('jane', 1), ('list', 1), ('sergeant', 1), ('store', 1), ('anoth', 1), ('assist', 1), ('black', 1), ('five', 1), ('major', 1)]\n",
      "Top 40 Female Words: [('dexter', 1), ('wright', 1), ('ladi', 1), ('phone', 1), ('interview', 1), ('wear', 1), ('dear', 1), ('wallac', 1), ('wilkin', 1), ('agent', 1), ('lawyer', 1), ('dawn', 1), ('fred', 1), ('home', 1), ('wonder', 1), ('elizabeth', 1), ('gibbon', 1), ('never', 1), ('munro', 1), ('knit', 1), ('letter', 1), ('met', 1), ('night', 1), ('student', 1), ('told', 1), ('tom', 1), ('watch', 1), ('enter', 1), ('mi', 1), ('perfum', 1), ('wig', 1), ('wild', 1), ('bag', 1), ('captain', 1), ('femal', 1), ('john', 1), ('project', 1), ('seen', 1), ('stay', 1), ('book', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Example word lists (assuming male_300 and female_300 are defined as strings)\n",
    "mwords = male_300.split()  # Male words\n",
    "fwords = female_300.split()  # Female words\n",
    "\n",
    "# Filtered words for male\n",
    "filteredMwords = [word for word in mwords if word in fm]\n",
    "wordMcounts = Counter(filteredMwords)\n",
    "topMwords = wordMcounts.most_common(40)\n",
    "print(\"Top 40 Male Words:\", topMwords)\n",
    "\n",
    "# Filtered words for female\n",
    "filteredFwords = [word for word in fwords if word in fw]\n",
    "wordFcounts = Counter(filteredFwords)\n",
    "topFwords = wordFcounts.most_common(40)\n",
    "print(\"Top 40 Female Words:\", topFwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9f779-7f69-4213-91cf-3429f1446bbf",
   "metadata": {},
   "source": [
    "# random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbdfc427-8744-4c54-ba73-fc4c9820b093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words most related to Female:\n",
      "           Word  FemaleImportance\n",
      "3374     wilkin               1.0\n",
      "720        dawn               1.0\n",
      "902   elizabeth               1.0\n",
      "1134       fred               1.0\n",
      "2366   princess               1.0\n",
      "1993      munro               1.0\n",
      "1270        guy               1.0\n",
      "764      dexter               1.0\n",
      "1899         mi               1.0\n",
      "1195     gibbon               1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "male_segments = fm.split(\" \")  \n",
    "female_segments = fw.split(\" \")  \n",
    "texts = male_segments + female_segments  \n",
    "labels = [0] * len(male_segments) + [1] * len(female_segments)  \n",
    "vectorizer = TfidfVectorizer()  \n",
    "X = vectorizer.fit_transform(texts)  \n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "clf.fit(X, labels)\n",
    "\n",
    "\n",
    "proba = clf.predict_proba(X)[:, 1]  # Probability of being female\n",
    "female_contributions = np.dot(X.T.toarray(), proba)\n",
    "female_importance = female_contributions / np.sum(X.toarray(), axis=0)\n",
    "female_importance_df = pd.DataFrame({\n",
    "    \"Word\": feature_names,\n",
    "    \"FemaleImportance\": female_importance\n",
    "}).sort_values(by=\"FemaleImportance\", ascending=False)\n",
    "\n",
    "print(\"Top words most related to Female:\")\n",
    "print(female_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41e753bf-6df0-45a0-8ec0-d4b191ff4127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words most related to Male:\n",
      "          Word  MaleImportance\n",
      "2893     store             1.0\n",
      "377      break             1.0\n",
      "2695       sgt             1.0\n",
      "1628      koca             1.0\n",
      "2864     stare             1.0\n",
      "1690    lester             1.0\n",
      "3096      took             1.0\n",
      "491   champion             1.0\n",
      "3411     worth             1.0\n",
      "1176      gave             1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "male_segments = fm.split(\" \")  \n",
    "female_segments = fw.split(\" \")  \n",
    "texts = male_segments + female_segments  \n",
    "labels = [0] * len(male_segments) + [1] * len(female_segments)  \n",
    "vectorizer = TfidfVectorizer()  \n",
    "X = vectorizer.fit_transform(texts)  \n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "clf.fit(X, labels)\n",
    "\n",
    "proba = clf.predict_proba(X)[:, 0]  # Probability of being male (class 0)\n",
    "\n",
    "\n",
    "male_contributions = np.dot(X.T.toarray(), proba)\n",
    "male_importance = male_contributions / np.sum(X.toarray(), axis=0)\n",
    "male_importance_df = pd.DataFrame({\n",
    "    \"Word\": feature_names,\n",
    "    \"MaleImportance\": male_importance\n",
    "}).sort_values(by=\"MaleImportance\", ascending=False)\n",
    "\n",
    "print(\"Top words most related to Male:\")\n",
    "print(male_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65eaaecb-a8fb-489c-b839-8eb044ce9925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm = ', '.join(map(str, [item for sublist in menParaOriginal for item in sublist]))\n",
    "fw = ', '.join(map(str, [item for sublist in womenParaOriginal for item in sublist]))\n",
    "ps = PorterStemmer()\n",
    "words = word_tokenize(fw)\n",
    "matching = [word for word in words if ps.stem(word) in {\"cass\", \"aborigin\",\"sungen\"}]\n",
    "matching"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
