{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392d2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    # Iterate over each page\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1042a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#textbooks that can be directly extracted:\n",
    "phili_7 = extract_text_from_pdf('Phili_7.pdf')\n",
    "phili_8 = extract_text_from_pdf('phili_8.pdf')\n",
    "phili_9 = extract_text_from_pdf('Phili_9.pdf')\n",
    "india_7 = extract_text_from_pdf('India_7.pdf')\n",
    "india_8 = extract_text_from_pdf('India_8.pdf')\n",
    "india_9 = extract_text_from_pdf('India_9.pdf')\n",
    "china_7 = extract_text_from_pdf('China_7.pdf')\n",
    "china_8 = extract_text_from_pdf('china_8.pdf')\n",
    "china_9 = extract_text_from_pdf('china_9.pdf')\n",
    "turkey_9 = extract_text_from_pdf('Turkey_9.pdf')\n",
    "tun_8 = extract_text_from_pdf('Tun_8.pdf')\n",
    "tun_9 = extract_text_from_pdf('Tun_9.pdf')\n",
    "saudi_7 = extract_text_from_pdf('Saudi_7.pdf')\n",
    "saudi_8 = extract_text_from_pdf('Saudi_8.pdf')\n",
    "saudi_9 = extract_text_from_pdf('Saudi_9.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "909494d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "textbooks = [phili_7, phili_8, phili_9, \n",
    "             india_7, india_8, india_9, \n",
    "             china_7, china_8, china_9, \n",
    "             turkey_9, \n",
    "             tun_8, tun_9, \n",
    "             saudi_7,saudi_8, saudi_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c440f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "phili = [phili_7, phili_8, phili_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51994bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "india = [india_7, india_8, india_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea2a4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "china = [china_7, china_8, china_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4ecec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "turkey = [turkey_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35594e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tun = [tun_8, tun_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76527851",
   "metadata": {},
   "outputs": [],
   "source": [
    "saudi = [saudi_7,saudi_8, saudi_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6e28ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3312cda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tal012/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tal012/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/tal012/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import natural language toolkit\n",
    "import nltk\n",
    "\n",
    "# download stopwords & punkt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# get lexicon we'll be working with today\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7aac2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(post):\n",
    "    if detect(post) != 'en':\n",
    "        return ''\n",
    "    # Make posts lowercase\n",
    "    post = post.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    post = re.sub(r'[^a-zA-Z\\s]', ' ', post)\n",
    "    \n",
    "    # Remove words with repeated letters\n",
    "    #post = re.sub(r'([a-zA-Z])\\1+', r'\\1', post)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(post)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96008ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTextbooks = []\n",
    "for i in textbooks:\n",
    "    cleanTextbooks.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b7219ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f79b3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bff8f3",
   "metadata": {},
   "source": [
    "The previous are pasted from previous. From here, analyze the countries that have been colonized vs those have not been colonized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f148c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonized = [phili_7, phili_8, phili_9, \n",
    "             india_7, india_8, india_9, \n",
    "             tun_8, tun_9, \n",
    "             ]\n",
    "#countries that have been colonized in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67cc2972",
   "metadata": {},
   "outputs": [],
   "source": [
    "notColonized = [\n",
    "             china_7, china_8, china_9, \n",
    "             turkey_9, \n",
    "             saudi_7,saudi_8, saudi_9]\n",
    "#Countries that have not been colonized in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22af844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "innerColonized = [phili_7, phili_8, phili_9, \n",
    "             india_7, india_8, india_9, \n",
    "             ]\n",
    "#Countries that have been colonized by inner circle countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13cae4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "chinaPara = [[] for _ in range(3)]\n",
    "for i in range(0, 3):\n",
    "    paragraphs = china[i].split('\\n')  # Assuming paragraphs are separated by newlines\n",
    "    for paragraph in paragraphs:\n",
    "        if \"china\" in paragraph.lower() or \"chinese\" in paragraph.lower():  # Converts paragraph to lowercase for case-insensitive search\n",
    "            chinaPara[i].append(paragraph)\n",
    "cleanChina = []\n",
    "for i in china:\n",
    "    cleanChina.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8a64053",
   "metadata": {},
   "outputs": [],
   "source": [
    "philiPara = [[] for _ in range(3)]\n",
    "for i in range(0, 3):\n",
    "    paragraphs = phili[i].split('\\n')  # Assuming paragraphs are separated by newlines\n",
    "    for paragraph in paragraphs:\n",
    "        if \"philippines\" in paragraph.lower() or \"philippino\" in paragraph.lower():  # Converts paragraph to lowercase for case-insensitive search\n",
    "            philiPara[i].append(paragraph)\n",
    "cleanPhili = []\n",
    "for i in phili:\n",
    "    cleanPhili.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b2d8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiaPara = [[] for _ in range(3)]\n",
    "for i in range(0, 3):\n",
    "    paragraphs = india[i].split('\\n')  # Assuming paragraphs are separated by newlines\n",
    "    for paragraph in paragraphs:\n",
    "        if \"india\" in paragraph.lower() or \"indian\" in paragraph.lower():  # Converts paragraph to lowercase for case-insensitive search\n",
    "            indiaPara[i].append(paragraph)\n",
    "cleanIndia = []\n",
    "for i in india:\n",
    "    cleanIndia.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2608949",
   "metadata": {},
   "outputs": [],
   "source": [
    "turkeyPara = [[] for _ in range(1)]\n",
    "for i in range(0, 1):\n",
    "    paragraphs = turkey[i].split('\\n')  # Assuming paragraphs are separated by newlines\n",
    "    for paragraph in paragraphs:\n",
    "        if \"turkey\" in paragraph.lower() or \"turkish\" in paragraph.lower():  # Converts paragraph to lowercase for case-insensitive search\n",
    "            turkeyPara[i].append(paragraph)\n",
    "cleanTurkey = []\n",
    "for i in turkey:\n",
    "    cleanTurkey.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6f8f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunPara = [[] for _ in range(2)]\n",
    "for i in range(0, 2):\n",
    "    paragraphs = tun[i].split('\\n')  # Assuming paragraphs are separated by newlines\n",
    "    for paragraph in paragraphs:\n",
    "        if \"tunisia\" in paragraph.lower() :  # Converts paragraph to lowercase for case-insensitive search\n",
    "            tunPara[i].append(paragraph)\n",
    "cleanTun = []\n",
    "for i in tun:\n",
    "    cleanTun.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b762829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "saudiPara = [[] for _ in range(3)]\n",
    "for i in range(0, 3):\n",
    "    paragraphs = saudi[i].split('\\n')  # Assuming paragraphs are separated by newlines\n",
    "    for paragraph in paragraphs:\n",
    "        if \"saudi\" in paragraph.lower() or \"arabian\" in paragraph.lower():  # Converts paragraph to lowercase for case-insensitive search\n",
    "            saudiPara[i].append(paragraph)\n",
    "cleanSaudi = []\n",
    "for i in saudi:\n",
    "    cleanSaudi.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f9aefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanCountrySelf = [cleanChina,cleanTurkey,cleanSaudi, cleanIndia, cleanPhili, cleanTun]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcbd694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: {'neg': 0.074, 'neu': 0.704, 'pos': 0.222, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.057, 'neu': 0.738, 'pos': 0.205, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.066, 'neu': 0.753, 'pos': 0.18, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.092, 'neu': 0.729, 'pos': 0.179, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.099, 'neu': 0.705, 'pos': 0.196, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.078, 'neu': 0.724, 'pos': 0.197, 'compound': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in cleanCountrySelf:\n",
    "    sentiment = analyzer.polarity_scores(i)\n",
    "    #print(f\"Text: {i}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a145f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
