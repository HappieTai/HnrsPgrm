{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da3b557a",
   "metadata": {},
   "source": [
    "PDF to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed1de71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.24.9-cp39-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.9 (from PyMuPDF)\n",
      "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.24.9-cp39-none-manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
      "\u001b[33m  WARNING: The script pymupdf is installed in '/home/tal012/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed PyMuPDF-1.24.9 PyMuPDFb-1.24.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114e782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    # Iterate over each page\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0964fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#textbooks that can be directly extracted:\n",
    "phili_7 = extract_text_from_pdf('Phili_7.pdf')\n",
    "phili_8 = extract_text_from_pdf('phili_8.pdf')\n",
    "phili_9 = extract_text_from_pdf('Phili_9.pdf')\n",
    "india_7 = extract_text_from_pdf('India_7.pdf')\n",
    "india_8 = extract_text_from_pdf('India_8.pdf')\n",
    "india_9 = extract_text_from_pdf('India_9.pdf')\n",
    "china_7 = extract_text_from_pdf('China_7.pdf')\n",
    "china_8 = extract_text_from_pdf('china_8.pdf')\n",
    "china_9 = extract_text_from_pdf('china_9.pdf')\n",
    "turkey_9 = extract_text_from_pdf('Turkey_9.pdf')\n",
    "tun_8 = extract_text_from_pdf('Tun_8.pdf')\n",
    "tun_9 = extract_text_from_pdf('Tun_9.pdf')\n",
    "saudi_7 = extract_text_from_pdf('Saudi_7.pdf')\n",
    "saudi_8 = extract_text_from_pdf('Saudi_8.pdf')\n",
    "saudi_9 = extract_text_from_pdf('Saudi_9.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ecce0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "textbooks = [phili_7, phili_8, phili_9, \n",
    "             india_7, india_8, india_9, \n",
    "             china_7, china_8, china_9, \n",
    "             turkey_9, \n",
    "             tun_8, tun_9, \n",
    "             saudi_7,saudi_8, saudi_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c035a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d21af4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "    'Phili_7.pdf', 'Phili_8.pdf', 'Phili_9.pdf',\n",
    "    'India_7.pdf', 'India_8.pdf', 'India_9.pdf',\n",
    "    'China_7.pdf', 'China_8.pdf', 'China_9.pdf',\n",
    "    'Turkey_9.pdf', \n",
    "    'Tun_8.pdf', 'Tun_9.pdf',\n",
    "    'Saudi_7.pdf', 'Saudi_8.pdf', 'Saudi_9.pdf'\n",
    "]\n",
    "\n",
    "with open('textbooks.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['File Name', 'Content'])  # Writing the header\n",
    "    \n",
    "    for file_name, content in zip(file_names, textbooks):\n",
    "        writer.writerow([file_name, content])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27932e",
   "metadata": {},
   "source": [
    "Try sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0982c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "684630f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=1f47f66122b6d68025476cca74a26fe67667cd6703860be68b0b9aed5d2099fd\n",
      "  Stored in directory: /home/tal012/.cache/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "455bdd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c74e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tal012/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tal012/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/tal012/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import natural language toolkit\n",
    "import nltk\n",
    "\n",
    "# download stopwords & punkt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# get lexicon we'll be working with today\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51611e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(post):\n",
    "    if detect(post) != 'en':\n",
    "        return ''\n",
    "    # Make posts lowercase\n",
    "    post = post.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    post = re.sub(r'[^a-zA-Z\\s]', ' ', post)\n",
    "    \n",
    "    # Remove words with repeated letters\n",
    "    #post = re.sub(r'([a-zA-Z])\\1+', r'\\1', post)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(post)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dec5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTextbooks = []\n",
    "for i in textbooks:\n",
    "    cleanTextbooks.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5dcfab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e89aaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: {'neg': 0.093, 'neu': 0.735, 'pos': 0.172, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.066, 'neu': 0.691, 'pos': 0.243, 'compound': 0.9996}\n",
      "\n",
      "Sentiment: {'neg': 0.101, 'neu': 0.695, 'pos': 0.204, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.101, 'neu': 0.708, 'pos': 0.191, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.093, 'neu': 0.735, 'pos': 0.171, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.083, 'neu': 0.739, 'pos': 0.178, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.059, 'neu': 0.735, 'pos': 0.206, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.071, 'neu': 0.713, 'pos': 0.216, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.09, 'neu': 0.667, 'pos': 0.243, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.057, 'neu': 0.738, 'pos': 0.205, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.063, 'neu': 0.736, 'pos': 0.201, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.093, 'neu': 0.714, 'pos': 0.193, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.023, 'neu': 0.802, 'pos': 0.176, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.092, 'neu': 0.719, 'pos': 0.189, 'compound': 1.0}\n",
      "\n",
      "Sentiment: {'neg': 0.074, 'neu': 0.751, 'pos': 0.176, 'compound': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in cleanTextbooks:\n",
    "    sentiment = analyzer.polarity_scores(i)\n",
    "    #print(f\"Text: {i}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb0ead",
   "metadata": {},
   "source": [
    "find US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80590e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWord(textbook, keyword):\n",
    "    indexes = []\n",
    "    start = 0\n",
    "    while True:\n",
    "        index = textbook.find(keyword, start)\n",
    "        if index == -1:\n",
    "            break\n",
    "        indexes.append(index)\n",
    "        start = index + 1  # Move start to the next character after the current found index\n",
    "    return indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de929941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11562, 12978, 13935, 38192, 105088, 161186, 161228, 202827]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phili_7_americaAT = findWord(cleanTextbooks[0],\"america\")\n",
    "phili_7_americaAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68bf74a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[11562, 12978, 13935, 38192, 105088, 161186, 161228, 202827]],\n",
       " [[]],\n",
       " [[141948,\n",
       "   143489,\n",
       "   144097,\n",
       "   144749,\n",
       "   155316,\n",
       "   155830,\n",
       "   156053,\n",
       "   156199,\n",
       "   156301,\n",
       "   156588,\n",
       "   157170,\n",
       "   159076,\n",
       "   160451,\n",
       "   221654,\n",
       "   235319,\n",
       "   309431,\n",
       "   309695,\n",
       "   311477,\n",
       "   338270,\n",
       "   339121,\n",
       "   340457,\n",
       "   341580,\n",
       "   345858,\n",
       "   362969,\n",
       "   363906,\n",
       "   363944,\n",
       "   372396,\n",
       "   379269,\n",
       "   379314,\n",
       "   391470,\n",
       "   421654,\n",
       "   422220,\n",
       "   422265,\n",
       "   437297,\n",
       "   437672,\n",
       "   440428,\n",
       "   448160,\n",
       "   448205,\n",
       "   450576,\n",
       "   450672,\n",
       "   450694,\n",
       "   450723,\n",
       "   450791,\n",
       "   450806,\n",
       "   450909,\n",
       "   450969,\n",
       "   451391,\n",
       "   451528,\n",
       "   452277,\n",
       "   452454,\n",
       "   452526,\n",
       "   453061,\n",
       "   453777,\n",
       "   454455,\n",
       "   464975,\n",
       "   465005,\n",
       "   465086,\n",
       "   465177,\n",
       "   542249,\n",
       "   542304,\n",
       "   542426,\n",
       "   551232]],\n",
       " [[30392]],\n",
       " [[30878]],\n",
       " [[23110,\n",
       "   32711,\n",
       "   58263,\n",
       "   58507,\n",
       "   58669,\n",
       "   61087,\n",
       "   99622,\n",
       "   108885,\n",
       "   116765,\n",
       "   116837,\n",
       "   118683]],\n",
       " [[17080,\n",
       "   41471,\n",
       "   44578,\n",
       "   63331,\n",
       "   66226,\n",
       "   68213,\n",
       "   68358,\n",
       "   75483,\n",
       "   75528,\n",
       "   77419,\n",
       "   78627,\n",
       "   81579,\n",
       "   81630]],\n",
       " [[13068,\n",
       "   22423,\n",
       "   22961,\n",
       "   23916,\n",
       "   24366,\n",
       "   24746,\n",
       "   31081,\n",
       "   33223,\n",
       "   33429,\n",
       "   34448,\n",
       "   34548,\n",
       "   34977,\n",
       "   38238,\n",
       "   39371,\n",
       "   40370,\n",
       "   46331,\n",
       "   46852,\n",
       "   47171,\n",
       "   47603,\n",
       "   51907,\n",
       "   54436,\n",
       "   67188,\n",
       "   67394,\n",
       "   70842,\n",
       "   71272]],\n",
       " [[11765,\n",
       "   12042,\n",
       "   19110,\n",
       "   23941,\n",
       "   24272,\n",
       "   24569,\n",
       "   32939,\n",
       "   33081,\n",
       "   33409,\n",
       "   34146,\n",
       "   35669,\n",
       "   36386,\n",
       "   46277,\n",
       "   49958,\n",
       "   50694,\n",
       "   61058]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[51949]],\n",
       " [[42566, 68741, 68829]]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "americaAT = [[] for _ in range(14)]\n",
    "for i in range(0,14):\n",
    "    americaAT[i].append(findWord(cleanTextbooks[i],\"america\"))\n",
    "americaAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5df20f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[194009]],\n",
       " [[]],\n",
       " [[144561, 432689, 437164, 437346, 439645, 465133]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[32697, 58199, 60478, 97979, 98471, 101465]],\n",
       " [[44564]],\n",
       " [[23527]],\n",
       " [[12236, 19165]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[11519]],\n",
       " [[]],\n",
       " [[]]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitedAT = [[] for _ in range(14)]\n",
    "for i in range(0,14):\n",
    "    unitedAT[i].append(findWord(cleanTextbooks[i],\"united states\"))\n",
    "unitedAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac9b020c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1 The Filipino is basically of Malay stock with a sprinkling of Chinese, American, ',\n",
       "  'Chinese and Spanish elements as well. The history of American rule and contact with ',\n",
       "  '5. The history of American rule and contact with merchants and traders culminated ',\n",
       "  'the early part of the American Regime. ',\n",
       "  '1. A history of the Philippines was at first written by Americans.  ',\n",
       "  'speech about America. Nothing he could say about the Pilgrim Fathers and the ',\n",
       "  'American custom of feasting on turkey seemed interesting. I thought of the ',\n",
       "  'Cultural Center of the Philippines, Star City, Coconut Palace, the United States ',\n",
       "  'Filipino-American '],\n",
       " [],\n",
       " ['papers geared to African American and Christian audiences.  ',\n",
       "  'In Chicago, she helped develop numerous African American women and reform ',\n",
       "  'African American women to sign \"the call\" to form the NAACP in 1909. Although ',\n",
       "  'African American Intellectual Tradition. 1995: Oxford University Press.  ',\n",
       "  'Five score years ago, a great American, in whose symbolic shadow we stand ',\n",
       "  'is still languished in the corners of American society and finds himself an exile in his ',\n",
       "  'American was to fall heir. This note was a promise that all men, yes, black men as well ',\n",
       "  'of happiness. It is obvious today that America has defaulted on this promissory note, ',\n",
       "  'America has given the Negro people a bad check, a check which has come back ',\n",
       "  'We have also come to this hallowed spot to remind America of the fierce urgency ',\n",
       "  'There will be neither rest nor tranquility in America until the Negro is granted his ',\n",
       "  'today and tomorrow, I still have a dream. It is a dream deeply rooted in the American ',\n",
       "  'if America is to be a great nation, this must become true. So let freedom ring from the ',\n",
       "  'American ',\n",
       "  'from any single ethnicity: Japanese, Middle Eastern, Native American, African, or any ',\n",
       "  'his mother’s protests, Boolie hires a 60-year-old African-American driver ',\n",
       "  'on his grandmother and her African-American driver, Will Coleman.  ',\n",
       "  'America is lined up in the driveway waving their fountain pens and falling ',\n",
       "  'uneducated African American who is sixty.  ',\n",
       "  'her prejudice  against African Americans. Boolie, at last gives up. When ',\n",
       "  \"Hoke's linkage of prejudice against Jews and African Americans. Though \",\n",
       "  '                      African- American.  ',\n",
       "  '         Alfred Fox Uhry, an American playwright, screenwriter, and member of the ',\n",
       "  'Travis could play -- a part of her great “American Dream” which she nurtures with her ',\n",
       "  'American\\xa0guy>\\xa0',\n",
       "  'American\\xa0mother>\\xa0',\n",
       "  'It’s Morning Again in America! ',\n",
       "  'LINDA (trying to bring him out of it): Willy, dear, I got a new kind of American-type cheese today. It’s ',\n",
       "  'WILLY: Why do you get American when I like Swiss? ',\n",
       "  'WILLY: You and Hap and I, and I’ll show you all the towns. America is full of beautiful towns and fine, ',\n",
       "  'falsity of the American dream.  ',\n",
       "  'American-type cheese today. It’s whipped. ',\n",
       "  'Willy: Why do you get American when I like Swiss? ',\n",
       "  'United States working‐class youth subculture ',\n",
       "  'widespread and long-lasting consequences for the United States. Historians still ',\n",
       "  'of American households were invested in the stock market within the United ',\n",
       "  'affects job security for employees, and as the American worker (the consumer) ',\n",
       "  \"Milton Friedman's A Monetary History of the United States, co-written with \",\n",
       "  '9. How does the Great Depression connect with the American Dream? ',\n",
       "  'American-type cheese today. It’s whipped. ',\n",
       "  'Willy: Why do you get American when I like Swiss? ',\n",
       "  'height of American capitalism, what do you think ',\n",
       "  '6. “Death of A Salesman” is acclaimed by critics as the great American tragedy. ',\n",
       "  '7. American forefathers defined American Dream with a moral vision of liberty, ',\n",
       "  'equality and truth. Compare and contrast the American Dream of America’s ',\n",
       "  'Read the video transcript of America’s President Barack Obama. Find out the ',\n",
       "  'issue he has presented in his message to the American people. ',\n",
       "  'of crisis and uncertainty have lifted, we need to focus on what the majority of Americans ',\n",
       "  ' President Barack Obama’s Address to the Americans ',\n",
       "  'already a broad coalition across America that’s behind this effort, from business leaders ',\n",
       "  'House should, too. The majority of Americans thinks this is the right thing to do. It can ',\n",
       "  'Third, we should pass a farm bill – one that America’s farmers and ranchers can ',\n",
       "  'our job the best we can. We come from different parties, but we’re Americans first. And ',\n",
       "  '1930’s in America? ',\n",
       "  'system. There’s already a broad coalition across America that’s behind this effort, from ',\n",
       "  '1. How is the American Dream characteristic of American ideals and philosophy?    ',\n",
       "  '    associated with the American Dream? ',\n",
       "  '2. What was happening economically and socially in the United States in 1949?  ',\n",
       "  '    Was it fairly easy or difficult to get a job? What was America’s standing in the  ',\n",
       "  'inability to balance her relationship with her children. What American values do ',\n",
       "  'you think the author wanted to convey, keep, or modify? How do American ',\n",
       "  '    twenty thousand dollars in his pocket?” to the American dream? ',\n",
       "  'Americans in the play? How do you compare this to your own value system? Be '],\n",
       " ['• ‘Fall’ (in American English) means the same as autumn,'],\n",
       " ['(a) American Declaration of Independence.'],\n",
       " ['choices that shape us. Robert Frost is an American poet who',\n",
       "  'the United States of America. He also took part in',\n",
       "  'Einstein emigrated to the United States.  Five years',\n",
       "  'American physicists in an uproar.  Many of them',\n",
       "  'to the American President, Franklin D. Roosevelt,',\n",
       "  'have an effect.  The Americans developed the atomic',\n",
       "  'Five years later, the discovery of nuclear fission in Berlin had American',\n",
       "  '– Why was Maria sent to the United States?',\n",
       "  'United States. That trip to Florida with her father',\n",
       "  'American accent, she proudly parades her Russian',\n",
       "  'off to train in the United States. (Tells us when Maria was sent to the U.S.)',\n",
       "  'the clear or breathy flutes of South America,',\n",
       "  '(American English) a person who',\n",
       "  '(American English) an informal way',\n",
       "  'Are you American, or is that merely a clever imitation?'],\n",
       " ['Russia, Korea, and America. My favorite food is pizza. It is from ',\n",
       "  '     B: Oh, really? Is she American?',\n",
       "  '例U. S. A. stands for United States of America. “U. S. A.”',\n",
       "  'American English Words',\n",
       "  'In America, they hold their hand out with their palm up and move their index',\n",
       "  'The biggest difference between American and British English is the words.',\n",
       "  'British people also speak with a different pronunciation from American people.',\n",
       "  'The Differences between American English and British English',\n",
       "  'The Differences between American',\n",
       "  'lives in America.',\n",
       "  'Nigel: Ahh, yes. Americans say French fries. In',\n",
       "  'between American',\n",
       "  'between American'],\n",
       " ['•  Susan says that Columbus discovered America in 1492.',\n",
       "  'was made by an American man who built things with wood. ',\n",
       "  '\\u3000\\u3000More than a million Flowbees were sold in America over the next ten years. ',\n",
       "  '3.  Flowbees are sold only in the United States. ',\n",
       "  '2.  More than a million Flowbees were sold in America over the next ten years.',\n",
       "  '•  The Flowbee was made by an American man.',\n",
       "  '4.  America ______________ in 1492. (discover)',\n",
       "  '2.  The Flowbee was made by an American man who built things with wood.',\n",
       "  'America for many years. I have also done ',\n",
       "  'What kind of volunteer work did you do in America?',\n",
       "  '3.  Susan’s mother did various kinds of volunteer work in America.  ',\n",
       "  'America',\n",
       "  '1.  I used to be a volunteer in our hometown in America for many years.  ',\n",
       "  'America on April 22, 1970. The 30th anniversary of Earth Day was in 2000. At that ',\n",
       "  'The first Earth Day started in America on April 22, _________________.',\n",
       "  '   thousands of schools and hundreds of communities across America.',\n",
       "  '1.  Americans _______________ Thanksgiving Day on the fourth Thursday in',\n",
       "  '      thousands of schools and hundreds of communities across America on April',\n",
       "  'B: I think so. _______________ the American',\n",
       "  '1.  The people _______________ live next door are Americans.',\n",
       "  'I used to be a volunteer in our hometown in America for many years.',\n",
       "  '（1）They met Dandan who visited America last year.',\n",
       "  'hometown in America for many years. I have also done this in other ',\n",
       "  'What kind of volunteer work did you do in America?',\n",
       "  '例Tom didn蒺t return to America until 2002.',\n",
       "  'In some American hospitals, dogs visit lonely patients so that they feel less lonely.'],\n",
       " ['A: Are you American?',\n",
       "  'Imagine you are American and you have just arrived in China.',\n",
       "  'Why are you visiting the United States?',\n",
       "  'American food?',\n",
       "  'the United States. What ',\n",
       "  '\\u3000\\u3000Martin Luther King was a famous African-American. He was born on January 15, ',\n",
       "  '\\u3000\\u3000At this time, African-Americans didn’t have the same 3. _________ as other people. ',\n",
       "  '5. _________ resist unfair things. His courageous fight for African-American rights set an ',\n",
       "  'The Americans gave the Soviets detailed photos of ',\n",
       "  'the Americans and Soviets together as teammates.',\n",
       "  'b. To compare Americans and Soviets.',\n",
       "  '     Americans?',\n",
       "  'some American audience believed it was real! Welles told the audience at the start of ',\n",
       "  '1. Why did some American listeners believe The War of the Worlds radio program was real?',\n",
       "  '\\u3000\\u3000In 1884, there was another Red Cross meeting in Geneva. An American woman named ',\n",
       "  'a generation ago, American parents tried to change their children’s left-handedness, even ',\n",
       "  '     b. American teachers forced students to use their left hands.',\n",
       "  '... , but saving the whales brought the Americans and Soviets together as teammates.'],\n",
       " ['America. It is the eleventh ',\n",
       "  'Korean-American family ',\n",
       "  'live the American dream, but ',\n",
       "  'the world, such as North and South America ',\n",
       "  '  a Meso-American masterpiece',\n",
       "  '[1] J. Bradbery, Ed., Oxford Basic American Dictionary for Learners of English, 1st ed., New York, the USA: Oxford \\t \\t'],\n",
       " [],\n",
       " ['times each year in the United States, dogs bite someone..'],\n",
       " ['   No         Penang is in South America.'],\n",
       " ['sport. It is much more popular than American ',\n",
       "  'people say American football is more popular. Others say baseball is more ',\n",
       "  'Both American football and baseball have big crowds at their games, but many ']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "americaPara = [[] for _ in range(14)]\n",
    "for i in range(0, 14):\n",
    "    paragraphs = textbooks[i].split('\\n')  # Assuming paragraphs are separated by newlines\n",
    "    for paragraph in paragraphs:\n",
    "        if \"america\" in paragraph.lower() or \"united states\" in paragraph.lower():  # Converts paragraph to lowercase for case-insensitive search\n",
    "            americaPara[i].append(paragraph)\n",
    "americaPara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea0ec952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: {'neg': 0.0, 'neu': 0.926, 'pos': 0.074, 'compound': 0.765}\n",
      "\n",
      "Sentiment: {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Sentiment: {'neg': 0.033, 'neu': 0.835, 'pos': 0.132, 'compound': 0.9954}\n",
      "\n",
      "Sentiment: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Sentiment: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Sentiment: {'neg': 0.0, 'neu': 0.844, 'pos': 0.156, 'compound': 0.9626}\n",
      "\n",
      "Sentiment: {'neg': 0.0, 'neu': 0.872, 'pos': 0.128, 'compound': 0.9013}\n",
      "\n",
      "Sentiment: {'neg': 0.019, 'neu': 0.97, 'pos': 0.011, 'compound': -0.3109}\n",
      "\n",
      "Sentiment: {'neg': 0.056, 'neu': 0.909, 'pos': 0.035, 'compound': -0.5502}\n",
      "\n",
      "Sentiment: {'neg': 0.0, 'neu': 0.964, 'pos': 0.036, 'compound': 0.128}\n",
      "\n",
      "Sentiment: {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Sentiment: {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'compound': 0.4215}\n",
      "\n",
      "Sentiment: {'neg': 0.306, 'neu': 0.694, 'pos': 0.0, 'compound': -0.296}\n",
      "\n",
      "Sentiment: {'neg': 0.0, 'neu': 0.883, 'pos': 0.117, 'compound': 0.4754}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in americaPara:\n",
    "    sentiment = analyzer.polarity_scores(i)\n",
    "    #print(f\"Text: {i}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205d048",
   "metadata": {},
   "source": [
    "UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41495d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[7354]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[79969, 83960]],\n",
       " [[]],\n",
       " [[63753, 98563]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[161, 791, 870]],\n",
       " [[161, 790, 869]]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ukAT = [[] for _ in range(14)]\n",
    "for i in range(0,14):\n",
    "    ukAT[i].append(findWord(cleanTextbooks[i],\" uk \"))\n",
    "ukAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "688814dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[26169]],\n",
       " [[]],\n",
       " [[79973, 83964]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[35775, 41515]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitedkAT = [[] for _ in range(14)]\n",
    "for i in range(0,14):\n",
    "    unitedkAT[i].append(findWord(cleanTextbooks[i],\"united kingdom\"))\n",
    "unitedkAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72d896e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[]],\n",
       " [[]],\n",
       " [[441860]],\n",
       " [[70717]],\n",
       " [[18150,\n",
       "   20870,\n",
       "   25773,\n",
       "   29024,\n",
       "   33925,\n",
       "   42087,\n",
       "   42168,\n",
       "   42348,\n",
       "   42506,\n",
       "   42776,\n",
       "   42802,\n",
       "   43016,\n",
       "   43119,\n",
       "   43287,\n",
       "   43757,\n",
       "   43896,\n",
       "   43940,\n",
       "   44041,\n",
       "   44136,\n",
       "   44181,\n",
       "   44514,\n",
       "   44671,\n",
       "   44690,\n",
       "   44836,\n",
       "   45847,\n",
       "   46345,\n",
       "   46652,\n",
       "   46726,\n",
       "   46977,\n",
       "   47921,\n",
       "   48113]],\n",
       " [[]],\n",
       " [[63354, 68137, 68222, 68308, 75500, 75545, 78700, 81187, 81588, 81639]],\n",
       " [[84789]],\n",
       " [[]],\n",
       " [[56004, 58153]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[50185]]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "britiAT = [[] for _ in range(14)]\n",
    "for i in range(0,14):\n",
    "    britiAT[i].append(findWord(cleanTextbooks[i],\"briti\"))\n",
    "britiAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe56c73",
   "metadata": {},
   "source": [
    "Try regular expression \\\n",
    "https://www.dataquest.io/blog/regex-cheatsheet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f505c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eafbe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"([^.]*\\bAmerica\\b[^.]*\\bbut\\b[^.]*\\.)|([^.]*\\bbut\\b[^.]*\\bAmerica\\b[^.]*\\.)\"\n",
    "for i in range(0,14):\n",
    "    matches = re.findall(pattern, cleanTextbooks[i], re.IGNORECASE)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e2384",
   "metadata": {},
   "source": [
    "# things that dont work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2f08a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3734\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "us =0\n",
    "unitedS=0\n",
    "america=0\n",
    "for i in cleanTextbooks:\n",
    "    for j in range(0, len(i), 2):\n",
    "        if(i[j:j+2]=='us'):\n",
    "            us+=1\n",
    "    for j in range(0, len(i), 13):\n",
    "        if(i[j:j+8]=='united states'):\n",
    "            unitedS+=1\n",
    "    for j in range(0, len(i), 7):\n",
    "        if(i[j:j+2]==\"america\"):\n",
    "            america+=1\n",
    "print(us)\n",
    "print(unitedS)\n",
    "print(america)\n",
    "#This is definitely not correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53176e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'www.minh-pham.info\\nwww.minh-pham.info\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viet_7 = extract_text_from_pdf('Viet_7.pdf')\n",
    "viet_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80802097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'www.minh-pham.info\\nwww.minh-pham.info\\nwww.minh-pham.info\\nwww.minh-pham.info\\nwww.minh-pham.info\\nwww.minh-pham.info\\nwww.minh-pham.info\\nwww.minh-pham.info\\nwww.minh-pham.info\\nwww.minh-pham.info\\nwww.minh-pham.info\\nwww.minh-pham.info\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viet_8 = extract_text_from_pdf('Viet_8.pdf')\n",
    "viet_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52826835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viet_9 = extract_text_from_pdf('Viet_9.pdf')\n",
    "viet_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33ed0991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mexico_7 = extract_text_from_pdf('Mexico_7.pdf')\n",
    "mexico_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b18d275f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mexico_8 = extract_text_from_pdf('Mexico_8.pdf')\n",
    "mexico_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18508171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tun_7 = extract_text_from_pdf('Tun_7.pdf')\n",
    "tun_7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe8a7a",
   "metadata": {},
   "source": [
    "Textbooks that cannot be directly extracted: ocr needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e118c",
   "metadata": {},
   "source": [
    "doctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e066fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting doctr\n",
      "  Downloading doctr-1.9.0-py3-none-any.whl.metadata (594 bytes)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from doctr) (5.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from doctr) (2.26.0)\n",
      "Requirement already satisfied: cryptography in /opt/conda/lib/python3.9/site-packages (from doctr) (3.4.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.9/site-packages (from cryptography->doctr) (1.14.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->doctr) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->doctr) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->doctr) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->doctr) (3.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography->doctr) (2.20)\n",
      "Downloading doctr-1.9.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: doctr\n",
      "\u001b[33m  WARNING: The script doctr is installed in '/home/tal012/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed doctr-1.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install doctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94060853",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'doctr.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_121/3352505144.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdoctr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mocr_predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdoctr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocumentFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'doctr.models'"
     ]
    }
   ],
   "source": [
    "from doctr.models import ocr_predictor\n",
    "from doctr.io import DocumentFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65675bef",
   "metadata": {},
   "source": [
    "Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adbd999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting keras-ocr\n",
      "  Downloading keras_ocr-0.9.3-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting editdistance (from keras-ocr)\n",
      "  Downloading editdistance-0.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting efficientnet==1.0.0 (from keras-ocr)\n",
      "  Downloading efficientnet-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting essential_generators (from keras-ocr)\n",
      "  Downloading essential_generators-1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting fonttools (from keras-ocr)\n",
      "  Downloading fonttools-4.53.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting imgaug (from keras-ocr)\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pyclipper (from keras-ocr)\n",
      "  Downloading pyclipper-1.3.0.post5-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: shapely in /home/tal012/.local/lib/python3.9/site-packages (from keras-ocr) (2.0.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from keras-ocr) (4.61.2)\n",
      "Collecting validators (from keras-ocr)\n",
      "  Downloading validators-0.33.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet==1.0.0->keras-ocr)\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.9/site-packages (from efficientnet==1.0.0->keras-ocr) (0.18.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from imgaug->keras-ocr) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/tal012/.local/lib/python3.9/site-packages (from imgaug->keras-ocr) (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/tal012/.local/lib/python3.9/site-packages (from imgaug->keras-ocr) (1.12.0)\n",
      "Requirement already satisfied: Pillow in /home/tal012/.local/lib/python3.9/site-packages (from imgaug->keras-ocr) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from imgaug->keras-ocr) (3.4.2)\n",
      "Collecting opencv-python (from imgaug->keras-ocr)\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: imageio in /opt/conda/lib/python3.9/site-packages (from imgaug->keras-ocr) (2.9.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.9/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0->keras-ocr) (3.3.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/tal012/.local/lib/python3.9/site-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (3.2.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.9/site-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (2021.7.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->imgaug->keras-ocr) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->imgaug->keras-ocr) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->imgaug->keras-ocr) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib->imgaug->keras-ocr) (2.8.2)\n",
      "Downloading keras_ocr-0.9.3-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
      "Downloading editdistance-0.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.6/401.6 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading essential_generators-1.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fonttools-4.53.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyclipper-1.3.0.post5-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (674 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m674.2/674.2 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading validators-0.33.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyclipper, essential_generators, validators, opencv-python, fonttools, editdistance, keras-applications, imgaug, efficientnet, keras-ocr\n",
      "\u001b[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/home/tal012/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed editdistance-0.8.1 efficientnet-1.0.0 essential_generators-1.0 fonttools-4.53.1 imgaug-0.4.0 keras-applications-1.0.8 keras-ocr-0.9.3 opencv-python-4.10.0.84 pyclipper-1.3.0.post5 validators-0.33.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a9ff68",
   "metadata": {},
   "source": [
    "Tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2252c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tesseract\n",
      "  Downloading tesseract-0.1.3.tar.gz (45.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: tesseract\n",
      "  Building wheel for tesseract (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tesseract: filename=tesseract-0.1.3-py3-none-any.whl size=45562569 sha256=850897efccb7f69a4a7803f5465b12f844857554590eb9ca4f759951db92a434\n",
      "  Stored in directory: /home/tal012/.cache/pip/wheels/6c/c5/81/8310cc52076953e53412ed1875a5e224c92940235bdcee21a2\n",
      "Successfully built tesseract\n",
      "Installing collected packages: tesseract\n",
      "Successfully installed tesseract-0.1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "476497e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytesseract in /home/tal012/.local/lib/python3.9/site-packages (0.3.10)\n",
      "Requirement already satisfied: PyMuPDF in /home/tal012/.local/lib/python3.9/site-packages (1.24.9)\n",
      "Requirement already satisfied: Pillow in /home/tal012/.local/lib/python3.9/site-packages (10.3.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.9/site-packages (from pytesseract) (23.2)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.9 in /home/tal012/.local/lib/python3.9/site-packages (from PyMuPDF) (1.24.9)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract PyMuPDF Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a35d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4699d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'Viet_8.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05b7aca1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TesseractNotFoundError",
     "evalue": "tesseract is not installed or it's not in your PATH. See README file for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubprocess_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1820\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tesseract'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTesseractNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_121/3120616277.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Perform OCR on the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Append the extracted text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mimage_to_string\u001b[0;34m(image, lang, config, nice, output_type, timeout)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     return {\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBYTES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBYTES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTRING\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     }[output_type]()\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_and_get_output\u001b[0;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[1;32m    286\u001b[0m         }\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mrun_tesseract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{kwargs['output_filename_base']}{extsep}{extension}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pytesseract/pytesseract.py\u001b[0m in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTesseractNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtimeout_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror_string\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTesseractNotFoundError\u001b[0m: tesseract is not installed or it's not in your PATH. See README file for more information."
     ]
    }
   ],
   "source": [
    "# Open the PDF file\n",
    "pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "# Initialize a string to store the extracted text\n",
    "extracted_text = \"\"\n",
    "\n",
    "# Iterate over each page\n",
    "for page_num in range(pdf_document.page_count):\n",
    "    page = pdf_document.load_page(page_num)\n",
    "    \n",
    "    # Render page to an image (pixmap)\n",
    "    pix = page.get_pixmap()\n",
    "    \n",
    "    # Convert the pixmap to an image\n",
    "    img = Image.open(io.BytesIO(pix.tobytes()))\n",
    "    \n",
    "    # Perform OCR on the image\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    \n",
    "    # Append the extracted text\n",
    "    extracted_text += f\"\\n\\nPage {page_num + 1}\\n{'=' * 20}\\n{text}\"\n",
    "\n",
    "# Close the PDF file\n",
    "pdf_document.close()\n",
    "\n",
    "# Print the extracted text\n",
    "print(extracted_text)\n",
    "\n",
    "# Save the extracted text to a file\n",
    "with open('extracted_text.txt', 'w') as text_file:\n",
    "    text_file.write(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d81f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
