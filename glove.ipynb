{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7f5044-7056-4b33-a576-cc93be3b6e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_427/1923593578.py:7: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_file = \"glove.6B.300d.txt\"\n",
    "word2vec_output_file = \"glove.6B.300d.word2vec\"\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, word2vec_output_file)\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0073d7-31d3-44f7-97f3-9adfe7ec0970",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Worthy Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "011c09f5-ba12-4bd9-b0b6-2eb9a1986369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('male', 0.894266664981842), ('woman', 0.6251603364944458), ('women', 0.6183491349220276), ('adult', 0.54055255651474), ('females', 0.5260580778121948), ('young', 0.5162490606307983), ('males', 0.5029097199440002), ('sex', 0.5002586245536804), ('men', 0.4739665389060974), ('african-american', 0.46953585743904114), ('sexual', 0.4482203423976898), ('girls', 0.4464503526687622), ('teenage', 0.4347599446773529), ('athlete', 0.4341106116771698), ('pregnant', 0.42897751927375793), ('gender', 0.4268523156642914), ('dancers', 0.41661420464515686), ('age', 0.41202616691589355), ('who', 0.4117957651615143), ('youngest', 0.40321215987205505), ('athletes', 0.40312257409095764), ('girl', 0.40003475546836853), ('offspring', 0.39921969175338745), ('child', 0.3980090022087097), ('whom', 0.3977397382259369), ('black', 0.39547640085220337), ('among', 0.3949969410896301), ('sexually', 0.39281994104385376), ('one', 0.39167311787605286), ('couples', 0.3895740509033203), ('younger', 0.3892822861671448), ('aged', 0.38915273547172546), ('older', 0.38909950852394104), ('singers', 0.38847145438194275), ('actresses', 0.38457709550857544), ('only', 0.38451895117759705), ('individuals', 0.3812375068664551), ('trainees', 0.3796015679836273), ('present', 0.3760262131690979), ('her', 0.37539225816726685), ('adults', 0.3733835518360138), ('herself', 0.37307268381118774), ('performer', 0.37232697010040283), ('person', 0.37126970291137695), ('sexes', 0.3712019622325897), ('performers', 0.3711831867694855), ('homosexual', 0.36943092942237854), ('dressed', 0.36767590045928955), ('actress', 0.36707261204719543), ('teen', 0.3668942153453827)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"female\", topn=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e99ca3ec-2d38-4599-bd72-1cfc13cefa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('female', 0.8942667245864868), ('males', 0.6204803586006165), ('adult', 0.5715945363044739), ('females', 0.5622289180755615), ('women', 0.5498304963111877), ('sex', 0.5298929214477539), ('woman', 0.5279442071914673), ('young', 0.5108012557029724), ('men', 0.5081120133399963), ('sexual', 0.48891758918762207), ('teenage', 0.46702226996421814), ('offspring', 0.4600728750228882), ('gender', 0.46005621552467346), ('girls', 0.45509791374206543), ('older', 0.45475679636001587), ('younger', 0.449379026889801), ('sexes', 0.44824710488319397), ('heterosexual', 0.4479052424430847), ('homosexual', 0.4399544298648834), ('age', 0.4351704716682434), ('adults', 0.4308137893676758), ('sexually', 0.4223162531852722), ('aged', 0.4213593006134033), ('unmarried', 0.41476866602897644), ('african-american', 0.41314056515693665), ('dancers', 0.40597018599510193), ('genitalia', 0.40547242760658264), ('adolescent', 0.39993542432785034), ('black', 0.3989153802394867), ('teenagers', 0.39429348707199097), ('pregnant', 0.3935745358467102), ('whereas', 0.3930467963218689), ('athlete', 0.392762154340744), ('couples', 0.38990408182144165), ('child', 0.3897799551486969), ('whom', 0.3872728645801544), ('only', 0.38684138655662537), ('athletes', 0.38638773560523987), ('boys', 0.3850289285182953), ('girl', 0.3820636570453644), ('masculine', 0.3815671503543854), ('mating', 0.38096821308135986), ('population', 0.3803027868270874), ('teen', 0.37980085611343384), ('wives', 0.3760838210582733), ('among', 0.37373918294906616), ('behavior', 0.3708858788013458), ('same', 0.369388610124588), ('sperm', 0.3677670359611511), ('singers', 0.3668944537639618)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"male\", topn = 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f36308d-3862-493b-9f5b-129aaf1ef6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nurses', 0.6968761086463928), ('nursing', 0.6000657081604004), ('doctor', 0.5859817862510681), ('physician', 0.5504167079925537), ('midwife', 0.5483213067054749), ('hospital', 0.5382149815559387), ('dentist', 0.527701735496521), ('therapist', 0.518688976764679), ('patient', 0.5183095335960388), ('practitioner', 0.5168794393539429), ('pharmacist', 0.513848602771759), ('mother', 0.5061590671539307), ('doctors', 0.49044689536094666), ('anesthetist', 0.48607227206230164), ('teacher', 0.48573037981987), ('paramedic', 0.48176488280296326), ('worker', 0.47940677404403687), ('pregnant', 0.47666165232658386), ('care', 0.4645204544067383), ('medical', 0.4617377519607544), ('anesthesiologist', 0.4605582356452942), ('anesthetists', 0.4560103118419647), ('midwives', 0.4541836082935333), ('woman', 0.44963565468788147), ('surgeon', 0.449190229177475), ('pediatrician', 0.44804254174232483), ('schoolteacher', 0.44629669189453125), ('psychiatrist', 0.43875646591186523), ('pediatric', 0.4372805953025818), ('receptionist', 0.43712317943573), ('filipina', 0.43415164947509766), ('clinic', 0.432635098695755), ('technician', 0.4319958984851837), ('veterinarian', 0.431555837392807), ('psychologist', 0.43072623014450073), ('wife', 0.4292469918727875), ('resident', 0.4261218309402466), ('physicians', 0.4246032238006592), ('maid', 0.42459940910339355), ('sick', 0.4186103045940399), ('psychiatric', 0.416059672832489), ('patients', 0.41160503029823303), ('assistants', 0.41133564710617065), ('housekeeper', 0.40507906675338745), ('medic', 0.4026169776916504), ('hospice', 0.40211498737335205), ('accountant', 0.4020473062992096), ('girlfriend', 0.3998892307281494), ('sister', 0.39854177832603455), ('paramedics', 0.39829692244529724)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"nurse\", topn = 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b44a069-186e-46d2-8ab7-e4e6ea55ba67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('soldiers', 0.7162267565727234), ('wounded', 0.6503106355667114), ('policeman', 0.6371234655380249), ('army', 0.5879427194595337), ('killed', 0.55167555809021), ('serviceman', 0.5464208722114563), ('man', 0.5163716673851013), ('troops', 0.5148918628692627), ('policemen', 0.513936460018158), ('prisoner', 0.5128335356712341), ('sergeant', 0.5111556053161621), ('dead', 0.5027180910110474), ('marines', 0.5017305016517639), ('commander', 0.48934832215309143), ('military', 0.48513782024383545), ('officer', 0.48509183526039124), ('wounding', 0.48107367753982544), ('ambush', 0.478008896112442), ('woman', 0.4743628203868866), ('captured', 0.47423848509788513), ('civilians', 0.46726277470588684), ('comrades', 0.46194082498550415), ('colonel', 0.46123006939888), ('slain', 0.4602411985397339), ('solider', 0.4598744809627533), ('servicemen', 0.45771220326423645), ('guards', 0.45508652925491333), ('boy', 0.4525468349456787), ('sailor', 0.4508024752140045), ('forces', 0.448721319437027), ('corporal', 0.44704872369766235), ('peacekeeper', 0.446821928024292), ('afghan', 0.4452809989452362), ('grenade', 0.4444721043109894), ('another', 0.44313573837280273), ('wounds', 0.4426375925540924), ('injured', 0.44234326481819153), ('gunmen', 0.4402313530445099), ('killing', 0.43896719813346863), ('kidnapped', 0.4362860918045044), ('airman', 0.43588975071907043), ('platoon', 0.4350893497467041), ('ambushed', 0.4329786002635956), ('israeli', 0.4311666786670685), ('gilad', 0.430038183927536), ('roadside', 0.42803066968917847), ('patrol', 0.4276033639907837), ('combat', 0.4274376630783081), ('shot', 0.4255780279636383), ('infantry', 0.42429977655410767)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"soldier\", topn = 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7bdd7b-918b-46d6-9d2b-700fdf8e074e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clusterness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb13934c-798d-499b-952a-c7f2ccd2e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "female = [\"female\", \"tv\", \"sport\", \"susan\", \"today\", \"verb\", \"beauty\", \"lesson\", \"party\", \"ok\", \"early\", \"usual\", \"emily\", \"often\", \"baby\", \"dad\", \"peter\", \"stevenson\", \"yesterday\", \"hope\", \"marriage\", \"photo\", \"st\", \"adj\", \"michael\", \"weekend\", \"bag\", \"pair\", \"tom\", \"dress\", \"niece\", \"enjoy\", \"lyn\", \"mum\", \"arrive\", \"bad\", \"french\", \"power\", \"problem\", \"care\", \"ed\", \"london\", \"practice\", \"short\", \"someone\", \"train\", \"voice\", \"without\", \"busy\", \"dance\", \"done\", \"drink\", \"drive\", \"grammar\", \"near\", \"note\", \"sing\", \"sometime\", \"stand\", \"wash\", \"wonder\"]\n",
    "male = [\"prince\", \"romeo\", \"juliet\", \"arthur\", \"kill\", \"dead\", \"tree\", \"god\", \"ben\", \"wise\", \"water\", \"face\", \"lawrence\", \"order\", \"become\", \"shall\", \"april\", \"paris\", \"cloth\", \"england\", \"film\", \"sound\", \"carry\", \"heard\", \"soon\", \"found\", \"land\", \"leon\", \"open\", \"laugh\", \"hermit\", \"sword\", \"draft\", \"dream\", \"gave\", \"lo\", \"around\", \"cri\", \"goes\", \"head\", \"white\", \"hour\", \"later\", \"street\", \"might\", \"police\", \"task\", \"idea\", \"bed\", \"bedivere\", \"mark\", \"match\", \"met\", \"six\", \"summer\", \"true\", \"modred\", \"game\", \"martin\", \"news\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c275453-ea9a-4937-bf66-7043c1f9b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def load_glove_model(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_vectors = load_glove_model(\"glove.6B.300d.txt\")\n",
    "\n",
    "def average_pairwise_similarity(words, embeddings):\n",
    "    valid_words = [w for w in words if w in embeddings]\n",
    "    vectors = [embeddings[w] for w in valid_words]\n",
    "    \n",
    "    if len(vectors) < 2:\n",
    "        return None  # Not enough valid words\n",
    "\n",
    "    similarities = []\n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(i + 1, len(vectors)):\n",
    "            sim = 1 - cosine(vectors[i], vectors[j])\n",
    "            similarities.append(sim)\n",
    "\n",
    "    return np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "650283c2-e80f-47fc-8116-16f48164b937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14951851686816736 0.14314663481385062\n"
     ]
    }
   ],
   "source": [
    "avgSimilarityFemale = average_pairwise_similarity(female, glove_vectors)\n",
    "avgSimilarityMale = average_pairwise_similarity(male, glove_vectors)\n",
    "print(avgSimilarityFemale,avgSimilarityMale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bb15d57-bc12-417d-8512-5368775b315b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Female Similarity: 0.14951851686816736\n",
      "Shuffled Female Similarity Mean: 0.14459286818779868, Std: 0.013078638730594281\n",
      "Original Male Similarity: 0.14314663481385062\n",
      "Shuffled Male Similarity Mean: 0.1437172511019369, Std: 0.013241148377941667\n",
      "P-value for Female Similarity: 0.344\n",
      "P-value for Male Similarity: 0.515\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "\n",
    "# Combine the two lists\n",
    "all_words = female + male\n",
    "\n",
    "# Number of Monte Carlo iterations\n",
    "num_iterations = 1000\n",
    "\n",
    "# Store the results of shuffled similarities\n",
    "shuffled_female_similarities = []\n",
    "shuffled_male_similarities = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Shuffle the combined list\n",
    "    random.shuffle(all_words)\n",
    "    \n",
    "    # Split into two new groups\n",
    "    shuffled_female = all_words[:len(female)]\n",
    "    shuffled_male = all_words[len(female):]\n",
    "    \n",
    "    # Calculate average pairwise similarity for shuffled groups\n",
    "    sim_female = average_pairwise_similarity(shuffled_female, glove_vectors)\n",
    "    sim_male = average_pairwise_similarity(shuffled_male, glove_vectors)\n",
    "    \n",
    "    # Store the results\n",
    "    if sim_female is not None:\n",
    "        shuffled_female_similarities.append(sim_female)\n",
    "    if sim_male is not None:\n",
    "        shuffled_male_similarities.append(sim_male)\n",
    "\n",
    "# Calculate the mean and standard deviation of the shuffled similarities\n",
    "mean_shuffled_female = np.mean(shuffled_female_similarities)\n",
    "std_shuffled_female = np.std(shuffled_female_similarities)\n",
    "\n",
    "mean_shuffled_male = np.mean(shuffled_male_similarities)\n",
    "std_shuffled_male = np.std(shuffled_male_similarities)\n",
    "\n",
    "# Compare with the original values\n",
    "print(f\"Original Female Similarity: {avgSimilarityFemale}\")\n",
    "print(f\"Shuffled Female Similarity Mean: {mean_shuffled_female}, Std: {std_shuffled_female}\")\n",
    "\n",
    "print(f\"Original Male Similarity: {avgSimilarityMale}\")\n",
    "print(f\"Shuffled Male Similarity Mean: {mean_shuffled_male}, Std: {std_shuffled_male}\")\n",
    "\n",
    "# Calculate p-values (proportion of shuffled similarities more extreme than the original)\n",
    "p_value_female = np.mean(np.array(shuffled_female_similarities) >= avgSimilarityFemale)\n",
    "p_value_male = np.mean(np.array(shuffled_male_similarities) >= avgSimilarityMale)\n",
    "\n",
    "print(f\"P-value for Female Similarity: {p_value_female}\")\n",
    "print(f\"P-value for Male Similarity: {p_value_male}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd922e9-6fe8-4510-a6ed-e2f78df29889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1119969312872051 0.11028016077859286\n"
     ]
    }
   ],
   "source": [
    "# l2 distance\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def load_glove_model(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_vectors = load_glove_model(\"glove.6B.300d.txt\")\n",
    "\n",
    "def average_pairwise_similarity(words, embeddings):\n",
    "    valid_words = [w for w in words if w in embeddings]\n",
    "    vectors = [embeddings[w] for w in valid_words]\n",
    "    \n",
    "    if len(vectors) < 2:\n",
    "        return None  # Not enough valid words\n",
    "\n",
    "    distances = []\n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(i + 1, len(vectors)):\n",
    "            dist = euclidean(vectors[i], vectors[j])\n",
    "            # Invert the distance to make it a similarity measure\n",
    "            similarity = 1 / (1 + dist)  # Adding 1 to avoid division by zero\n",
    "            distances.append(similarity)\n",
    "\n",
    "    return np.mean(distances)\n",
    "avgSimilarityFemale = average_pairwise_similarity(female, glove_vectors)\n",
    "avgSimilarityMale = average_pairwise_similarity(male, glove_vectors)\n",
    "print(avgSimilarityFemale,avgSimilarityMale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98843591-ad24-44a7-a3e8-d72f33c501ea",
   "metadata": {},
   "source": [
    "# Distance of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a30cb6a-f038-43e6-a173-924a2d2658e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_distance(words, target_word, embeddings):\n",
    "    if target_word not in embeddings:\n",
    "        print(f\"Target word '{target_word}' not found in embeddings.\")\n",
    "        return None\n",
    "\n",
    "    target_vector = embeddings[target_word]\n",
    "    total_distance = 0\n",
    "    valid_word_count = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in embeddings:\n",
    "            sim = 1 - cosine(target_vector, embeddings[word])\n",
    "            distance = 1 - sim  # Cosine distance = 1 - similarity\n",
    "            total_distance += distance\n",
    "            valid_word_count += 1\n",
    "        else:\n",
    "            print(f\"Word '{word}' not found in embeddings.\")\n",
    "\n",
    "    if valid_word_count == 0:\n",
    "        print(\"No valid words found in embeddings.\")\n",
    "        return None\n",
    "\n",
    "    average_distance = total_distance / valid_word_count\n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80b28a2c-ee68-4091-9968-e82965e0c993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8217473852754117\n",
      "0.7590569027231464\n"
     ]
    }
   ],
   "source": [
    "print(compute_average_distance(female, \"death\" , glove_vectors))\n",
    "print(compute_average_distance(male, \"death\" , glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e6560cf-aa55-4795-a5bc-a6eeabe48bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8192254769830962\n",
      "0.8092537426011281\n"
     ]
    }
   ],
   "source": [
    "print(compute_average_distance(female, \"food\" , glove_vectors))\n",
    "print(compute_average_distance(male, \"food\" , glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "491964c4-66fb-4a0b-8ba5-4ce64df00ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7741998049498664\n",
      "0.8067489229886335\n"
     ]
    }
   ],
   "source": [
    "print(compute_average_distance(female, \"baby\" , glove_vectors))\n",
    "print(compute_average_distance(male, \"baby\" , glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4a4db31-cef3-49cb-82d6-acf0fb27db96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7466379320555575\n",
      "0.7713240791176033\n"
     ]
    }
   ],
   "source": [
    "print(compute_average_distance(female, \"pretty\" , glove_vectors))\n",
    "print(compute_average_distance(male, \"pretty\" , glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "760a3b8a-5d7f-462c-9a32-c7faaafde418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7392597785242665\n",
      "0.7472350933511307\n"
     ]
    }
   ],
   "source": [
    "print(compute_average_distance(female, \"love\" , glove_vectors))\n",
    "print(compute_average_distance(male, \"love\" , glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18f1fe2d-f449-453f-a466-70488bcabf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8556785941830776\n",
      "0.8219010466575281\n"
     ]
    }
   ],
   "source": [
    "print(compute_average_distance(female, \"violence\" , glove_vectors))\n",
    "print(compute_average_distance(male, \"violence\" , glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf548b7f-5c07-4d5a-bc9a-bcb86d81bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_l2_distance(words, target_word, embeddings):\n",
    "    if target_word not in embeddings:\n",
    "        print(f\"Target word '{target_word}' not found in embeddings.\")\n",
    "        return None\n",
    "\n",
    "    target_vector = embeddings[target_word]\n",
    "    total_distance = 0\n",
    "    valid_word_count = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in embeddings:\n",
    "            # Calculate L2 distance (Euclidean distance)\n",
    "            distance = np.linalg.norm(target_vector - embeddings[word])\n",
    "            total_distance += distance\n",
    "            valid_word_count += 1\n",
    "        else:\n",
    "            print(f\"Word '{word}' not found in embeddings.\")\n",
    "\n",
    "    if valid_word_count == 0:\n",
    "        print(\"No valid words found in embeddings.\")\n",
    "        return None\n",
    "\n",
    "    average_distance = total_distance / valid_word_count\n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73a99d1-847c-4957-81d3-2ea9f1b1ede2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.103905310396287\n",
      "7.966723132133484\n"
     ]
    }
   ],
   "source": [
    "print(compute_l2_distance(female, \"death\" , glove_vectors))\n",
    "print(compute_l2_distance(male, \"death\" , glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9974bef9-e654-45e5-a918-9b6c02bb4cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.530167947050002\n",
      "8.655700874328613\n"
     ]
    }
   ],
   "source": [
    "print(compute_l2_distance(female, \"food\" , glove_vectors))\n",
    "print(compute_l2_distance(male, \"food\" , glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc28e20e-59a3-412e-95f4-aa1a8fdada87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.023940563201904\n",
      "8.381506689389546\n"
     ]
    }
   ],
   "source": [
    "print(compute_l2_distance(female, \"baby\" , glove_vectors))\n",
    "print(compute_l2_distance(male, \"baby\" , glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d202e4c-a0c1-444c-984f-3281e64736b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.434715185009066\n",
      "7.708554848035177\n"
     ]
    }
   ],
   "source": [
    "print(compute_l2_distance(female, \"pretty\" , glove_vectors))\n",
    "print(compute_l2_distance(male, \"pretty\" , glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c9f42a-7a35-4de9-bd64-8e9207fff00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.511513225367812\n",
      "7.705495516459147\n"
     ]
    }
   ],
   "source": [
    "print(compute_l2_distance(female, \"love\" , glove_vectors))\n",
    "print(compute_l2_distance(male, \"love\" , glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b98b10b-498a-497e-a3f6-7ee4a08dbc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.511513225367812\n",
      "7.705495516459147\n"
     ]
    }
   ],
   "source": [
    "print(compute_l2_distance(female, \"love\" , glove_vectors))\n",
    "print(compute_l2_distance(male, \"love\" , glove_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28441e07-3479-4bd7-af69-fb581fed801f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Central words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95b06039-6f0f-4d7e-bdb9-69af8c32b0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tal012/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78e1d0d3-009e-490b-b832-9081e3921b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def compute_average_distance_to_cluster(word, cluster_words, embeddings):\n",
    "    \"\"\"Compute the average distance of a word to all words in a cluster.\"\"\"\n",
    "    if word not in embeddings:\n",
    "        return None\n",
    "\n",
    "    total_distance = 0\n",
    "    valid_word_count = 0\n",
    "\n",
    "    for cluster_word in cluster_words:\n",
    "        if cluster_word in embeddings:\n",
    "            sim = 1 - cosine(embeddings[word], embeddings[cluster_word])\n",
    "            distance = 1 - sim  # Cosine distance = 1 - similarity\n",
    "            total_distance += distance\n",
    "            valid_word_count += 1\n",
    "        else:\n",
    "            print(f\"Cluster word '{cluster_word}' not found in embeddings.\")\n",
    "\n",
    "    if valid_word_count == 0:\n",
    "        return None\n",
    "\n",
    "    return total_distance / valid_word_count\n",
    "\n",
    "def find_central_word(cluster_words, embeddings, search_in_vocab=False):\n",
    "    \"\"\"Find the word that is closest to all words in the cluster, excluding stopwords.\"\"\"\n",
    "    central_word = None\n",
    "    min_avg_distance = float('inf')\n",
    "\n",
    "    # If search_in_vocab is True, search the entire vocabulary for the central word\n",
    "    if search_in_vocab:\n",
    "        search_words = embeddings.keys()\n",
    "    else:\n",
    "        search_words = cluster_words\n",
    "\n",
    "    for word in search_words:\n",
    "        # Skip stopwords\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "\n",
    "        avg_distance = compute_average_distance_to_cluster(word, cluster_words, embeddings)\n",
    "        if avg_distance is not None and avg_distance < min_avg_distance:\n",
    "            min_avg_distance = avg_distance\n",
    "            central_word = word\n",
    "\n",
    "    return central_word, min_avg_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c61b5570-f74e-46ab-aa34-d1b905bbd08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central word (within female list): done, Average distance: 0.7208801886398108\n",
      "Central word (in entire vocabulary): n't, Average distance: 0.6714153385305046\n"
     ]
    }
   ],
   "source": [
    "# Find the central word within the female list\n",
    "central_word, avg_distance = find_central_word(female, glove_vectors, search_in_vocab=False)\n",
    "print(f\"Central word (within female list): {central_word}, Average distance: {avg_distance}\")\n",
    "\n",
    "# Find the central word in the entire vocabulary\n",
    "central_word, avg_distance = find_central_word(female, glove_vectors, search_in_vocab=True)\n",
    "print(f\"Central word (in entire vocabulary): {central_word}, Average distance: {avg_distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb3811b8-ec27-45bc-9c83-a3d4a9d0b227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central word (within female list): later, Average distance: 0.6779155654724714\n",
      "Central word (in entire vocabulary): even, Average distance: 0.640713315872982\n"
     ]
    }
   ],
   "source": [
    "# Find the central word within the female list\n",
    "central_word, avg_distance = find_central_word(male, glove_vectors, search_in_vocab=False)\n",
    "print(f\"Central word (within female list): {central_word}, Average distance: {avg_distance}\")\n",
    "\n",
    "# Find the central word in the entire vocabulary\n",
    "central_word, avg_distance = find_central_word(male, glove_vectors, search_in_vocab=True)\n",
    "print(f\"Central word (in entire vocabulary): {central_word}, Average distance: {avg_distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f08e763-6169-4ca7-ac53-64f46f93d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from nltk.corpus import stopwords\n",
    "import heapq\n",
    "\n",
    "def compute_average_distance_to_cluster(word, cluster_words, embeddings):\n",
    "    \"\"\"Compute the average distance of a word to all words in a cluster.\"\"\"\n",
    "    if word not in embeddings:\n",
    "        return None\n",
    "\n",
    "    total_distance = 0\n",
    "    valid_word_count = 0\n",
    "\n",
    "    for cluster_word in cluster_words:\n",
    "        if cluster_word in embeddings:\n",
    "            sim = 1 - cosine(embeddings[word], embeddings[cluster_word])\n",
    "            distance = 1 - sim  # Cosine distance = 1 - similarity\n",
    "            total_distance += distance\n",
    "            valid_word_count += 1\n",
    "        else:\n",
    "            print(f\"Cluster word '{cluster_word}' not found in embeddings.\")\n",
    "\n",
    "    if valid_word_count == 0:\n",
    "        return None\n",
    "\n",
    "    return total_distance / valid_word_count\n",
    "\n",
    "def find_top_central_words(cluster_words, embeddings, search_in_vocab=False, top_n=10):\n",
    "    \"\"\"Find the top N words that are closest to all words in the cluster, excluding stopwords.\"\"\"\n",
    "    central_words = []\n",
    "    min_heap = []  # Use a min-heap to keep track of the top N words with the smallest distances\n",
    "\n",
    "    # If search_in_vocab is True, search the entire vocabulary for the central word\n",
    "    if search_in_vocab:\n",
    "        search_words = embeddings.keys()\n",
    "    else:\n",
    "        search_words = cluster_words\n",
    "\n",
    "    for word in search_words:\n",
    "        # Skip stopwords\n",
    "        if word in stopwords.words('english'):\n",
    "            continue\n",
    "\n",
    "        avg_distance = compute_average_distance_to_cluster(word, cluster_words, embeddings)\n",
    "        if avg_distance is not None:\n",
    "            # Use a max-heap by storing negative distances\n",
    "            if len(min_heap) < top_n:\n",
    "                heapq.heappush(min_heap, (-avg_distance, word))\n",
    "            else:\n",
    "                # If the current word has a smaller distance than the largest in the heap, replace it\n",
    "                if -avg_distance > min_heap[0][0]:\n",
    "                    heapq.heappop(min_heap)\n",
    "                    heapq.heappush(min_heap, (-avg_distance, word))\n",
    "\n",
    "    # Extract the top N words from the heap\n",
    "    while min_heap:\n",
    "        dist, word = heapq.heappop(min_heap)\n",
    "        central_words.append((word, -dist))  # Convert back to positive distance\n",
    "\n",
    "    # Return the words in order of smallest distance to largest\n",
    "    central_words.reverse()\n",
    "    return central_words\n",
    "\n",
    "# Example usage:\n",
    "# embeddings = {\"word1\": [0.1, 0.2, ...], \"word2\": [0.3, 0.4, ...], ...}\n",
    "# cluster_words = [\"word1\", \"word2\", ...]\n",
    "# top_central_words = find_top_central_words(cluster_words, embeddings, search_in_vocab=True, top_n=10)\n",
    "# print(top_central_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0b69b3b-32e7-4873-93ef-9b49253797be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"n't\", 0.6714153385305046), ('even', 0.6728501777813096), ('come', 0.6753482568104408), ('get', 0.684200664528318), ('way', 0.6871654166127258), ('something', 0.6876067652028133), ('always', 0.6881692088176292), ('?', 0.6885415082749461), ('going', 0.6885898912276801), ('know', 0.6888076759117168), ('go', 0.6891552695421795), ('good', 0.6895323546665374), ('actually', 0.6912376619963497), (\"'ll\", 0.6914695738436394), ('time', 0.6933339809861555), ('really', 0.6944868975801137), ('kind', 0.6961081893655953), ('want', 0.6963673919880301), ('take', 0.6967014856398578), ('well', 0.6974806899694086), ('though', 0.6986354750016764), ('nothing', 0.6987508453355472), ('things', 0.7000544646583349), ('think', 0.7004141563375282), ('everyone', 0.701183063043642), ('.', 0.7025120276392267), ('one', 0.703272991982165), ('sure', 0.7046832524041194), ('maybe', 0.7049145392974244), ('make', 0.7053197179615756), (\"'d\", 0.7057848481895596), ('rather', 0.7064046601977039), ('thing', 0.7068652841077022), ('instead', 0.7068689062393254), ('like', 0.7069607523964526), ('everything', 0.7075274368234671), ('anything', 0.7078931530454621), ('might', 0.7093695443685077), ('else', 0.7094297416319946), ('look', 0.7100961708284778), ('need', 0.7106115834587777), ('never', 0.7108025381701398), ('fact', 0.7110314047879209), (\"'re\", 0.7124010527731807), ('coming', 0.7136335911444914), ('see', 0.7140495660346856), ('much', 0.7145069835745302), ('every', 0.7157037238168343), ('simply', 0.7163627344032628), ('lot', 0.7165739925962393), ('sort', 0.7176116154533579), ('tell', 0.7182094907602403), ('find', 0.7184872915945056), ('day', 0.7185906087728827), ('say', 0.7187941094670945), ('couple', 0.7194899576581635), ('probably', 0.719841022088617), ('would', 0.7203196347264644), ('could', 0.7204101194801552), ('done', 0.7208801886398108), ('better', 0.7210285488595068), ('next', 0.7227039547156394), ('ca', 0.7228268065706727), ('let', 0.723059531189966), ('little', 0.7234518586183596), ('thought', 0.7234528472120487), ('still', 0.7236129882709477), ('indeed', 0.7243211765597848), ('yet', 0.7246616854135832), ('another', 0.7247096081938695), ('without', 0.7252270757824744), ('days', 0.7254434087017783), ('got', 0.7256421887100676), (\"'ve\", 0.7256960860171241), ('put', 0.7265996500687516), ('comes', 0.7266301575282818), ('give', 0.7270950491409691), ('back', 0.72731419652187), ('enough', 0.727342938940879), ('getting', 0.7278867275265078), ('anyway', 0.7281152451653883), ('seen', 0.7283509139101955), ('reason', 0.728710638630251), (',', 0.7291536563163283), ('keep', 0.7292114413290326), ('wanted', 0.7294338066011063), ('rest', 0.7297323604642585), ('someone', 0.7300025233151166), ('either', 0.730585777473758), (\"'m\", 0.7307720871863063), ('wo', 0.7308679896693754), ('certainly', 0.731198462608836), ('able', 0.7314911411097661), ('today', 0.7316000773984813), ('talk', 0.7318963615871831), ('gone', 0.732257104447911), ('work', 0.73258675960455), ('perhaps', 0.7329930237792348), ('turn', 0.7330290720443609), ('although', 0.7331393729605992)]\n"
     ]
    }
   ],
   "source": [
    "top_central_words = find_top_central_words(female, glove_vectors, search_in_vocab=True, top_n=100)\n",
    "print(top_central_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a2e7df6-3a97-4714-9282-b717a2fd511d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('even', 0.640713315872982), ('.', 0.6425608691925747), ('come', 0.644528121010361), ('one', 0.6538608689999121), ('time', 0.659663861671417), ('though', 0.6616881406587689), ('would', 0.6633359742572603), (\"n't\", 0.6658809976609401), ('way', 0.6664170780554001), ('take', 0.6669557892343778), ('could', 0.6672502289257325), ('well', 0.6687135441662387), (',', 0.6718726196626932), ('go', 0.6721637763708513), ('actually', 0.6725507281077291), ('another', 0.6733054844145249), ('put', 0.673914177185209), ('get', 0.674352568604232), ('next', 0.6747233925327935), ('make', 0.6748979512602351), ('never', 0.674915448120292), ('came', 0.6769529725157235), ('however', 0.6777384424565985), ('later', 0.6779155654724714), ('instead', 0.6804128610271589), ('yet', 0.6808476575633569), ('going', 0.680926250745386), ('although', 0.6823174216625593), ('coming', 0.6825292838807594), ('back', 0.6830039098389775), ('might', 0.6837820025453559), ('soon', 0.6838968643988219), ('last', 0.6856532611630481), ('first', 0.6865055743651786), ('nothing', 0.6868147365549854), ('good', 0.6874902198699849), ('?', 0.6886964050548902), ('something', 0.688824417882518), ('day', 0.6903113047365732), ('fact', 0.6906102955556402), ('much', 0.6906787109915389), ('days', 0.6917421586321615), ('know', 0.6926069461261551), ('still', 0.6938145414413392), ('taken', 0.6938620852885574), ('like', 0.6942770790838746), ('also', 0.6952075619044202), ('kind', 0.6957433226804477), ('give', 0.6960705904079229), ('rather', 0.696307704116207), ('without', 0.6964035634307622), ('thought', 0.6971897198024335), ('always', 0.6975284184919129), ('every', 0.6976624120690655), ('turn', 0.6982687724202943), ('probably', 0.6985355966522926), ('must', 0.6989958810109675), ('finally', 0.7007088379357133), ('find', 0.700863343958667), ('look', 0.7015393688001595), ('wanted', 0.7020885567435079), ('others', 0.7024431514006237), ('say', 0.7025069121037456), ('want', 0.7030714577009859), ('man', 0.7033398327331507), ('rest', 0.70366900836224), ('given', 0.7037032825982563), (\"'ll\", 0.7041073024309056), ('made', 0.7042066236626634), ('able', 0.7045011932422003), ('see', 0.7046472354367064), ('night', 0.7047317020995435), ('enough', 0.7050248338999533), ('went', 0.7055173534286407), ('turned', 0.7057177330282154), ('indeed', 0.705940856662905), ('may', 0.7062072037153075), ('ever', 0.7067238754901087), ('making', 0.7068933704190802), ('either', 0.7071360302986713), ('years', 0.7072180110293257), ('week', 0.7076613818192703), ('everyone', 0.7077462510514381), ('already', 0.7079835933298153), ('think', 0.7079884712665745), ('saying', 0.7080993868102121), ('need', 0.7082922546040772), ('got', 0.7083562386596227), ('seen', 0.7096719965330663), ('things', 0.7099505856204836), ('part', 0.7109270935667675), ('sure', 0.711022344273014), ('many', 0.7111043585523746), ('people', 0.7116091492277075), ('saw', 0.7118530339521187), (\"'d\", 0.7118701109646567), ('three', 0.7121349808801806), ('comes', 0.7125493360941788), ('really', 0.7126385015749349), ('ago', 0.7132064608781463)]\n"
     ]
    }
   ],
   "source": [
    "top_central_words = find_top_central_words(male, glove_vectors, search_in_vocab=True, top_n=100)\n",
    "print(top_central_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca48e85b-aa77-4aee-ab85-1a1e9a254dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central word (within female list): done, Average distance: 0.7208801886398108\n",
      "Central word (in entire vocabulary): n't, Average distance: 0.6714153385305046\n"
     ]
    }
   ],
   "source": [
    "# Find the central word within the female list\n",
    "central_word, avg_distance = find_central_word(female, glove_vectors, search_in_vocab=False)\n",
    "print(f\"Central word (within female list): {central_word}, Average distance: {avg_distance}\")\n",
    "\n",
    "# Find the central word in the entire vocabulary\n",
    "central_word, avg_distance = find_central_word(female, glove_vectors, search_in_vocab=True)\n",
    "print(f\"Central word (in entire vocabulary): {central_word}, Average distance: {avg_distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4aa4edb-aec1-48e3-b398-5e0c82b9ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central word (within female list): later, Average distance: 0.6779155654724714\n",
      "Central word (in entire vocabulary): even, Average distance: 0.640713315872982\n"
     ]
    }
   ],
   "source": [
    "# Find the central word within the male list\n",
    "central_word, avg_distance = find_central_word(male, glove_vectors, search_in_vocab=False)\n",
    "print(f\"Central word (within female list): {central_word}, Average distance: {avg_distance}\")\n",
    "\n",
    "# Find the central word in the entire vocabulary\n",
    "central_word, avg_distance = find_central_word(male, glove_vectors, search_in_vocab=True)\n",
    "print(f\"Central word (in entire vocabulary): {central_word}, Average distance: {avg_distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f56f41d5-2776-4173-b99d-4210bc2b0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "female = [\"female\", \"tv\", \"sport\", \"today\", \"verb\", \"beauty\", \"lesson\", \"party\", \"ok\", \"early\", \"usual\", \"often\", \"baby\", \"dad\", \"yesterday\", \"hope\", \"marriage\", \"photo\", \"st\", \"adj\",\"weekend\", \"bag\", \"pair\", \"dress\", \"niece\", \"enjoy\",  \"mum\", \"arrive\", \"bad\", \"french\", \"power\", \"problem\", \"care\", \"ed\", \"london\", \"practice\", \"short\", \"someone\", \"train\", \"voice\", \"without\", \"busy\", \"dance\", \"done\", \"drink\", \"drive\", \"grammar\", \"near\", \"note\", \"sing\", \"sometime\", \"stand\", \"wash\", \"wonder\"]\n",
    "male = [\"prince\", \"kill\", \"dead\", \"tree\", \"god\", \"wise\", \"water\", \"face\", \"order\", \"become\", \"shall\", \"april\", \"paris\", \"cloth\", \"england\", \"film\", \"sound\", \"carry\", \"heard\", \"soon\", \"found\", \"land\", \"open\", \"laugh\",  \"sword\", \"draft\", \"dream\", \"gave\", \"lo\", \"around\", \"cri\", \"goes\", \"head\", \"white\", \"hour\", \"later\", \"street\", \"might\", \"police\", \"task\", \"idea\", \"bed\", \"match\", \"met\", \"six\", \"summer\", \"true\", \"game\",  \"news\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c280a-3c62-483e-9440-bd9cfb2943bf",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ceb1c-171a-4840-bd21-4056abc7bde3",
   "metadata": {},
   "source": [
    "https://www.europarl.europa.eu/cmsdata/151780/GNL_Guidelines_EN.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7601e-81c0-4c88-8e28-6100334b0faf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
